{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: mps (Mac GPU)\n",
      "PyTorch version: 2.9.0.dev20250704\n",
      "Using Mac Metal Performance Shaders for GPU acceleration\n",
      "PyTorch version: 2.9.0.dev20250704\n",
      "Using Mac Metal Performance Shaders for GPU acceleration\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')  # Mac Metal Performance Shaders\n",
    "    print(f\"Training on device: {device} (Mac GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Training on device: {device} (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Training on device: {device} (CPU)\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if device.type == 'mps':\n",
    "    print(\"Using Mac Metal Performance Shaders for GPU acceleration\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if device.type == 'mps':\n",
    "    print(\"Using Mac Metal Performance Shaders for GPU acceleration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Transformer layer for metabolic modeling\"\"\"\n",
    "    def __init__(self,vocab_size=115,dim=6,num_heads=2):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        assert dim%num_heads==0, \"model dimension must be divisible by number of heads\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = dim\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.k = dim//num_heads\n",
    "\n",
    "        self.W_k = nn.Linear(dim,self.k,bias=False)\n",
    "        self.W_q = nn.Linear(dim,self.k,bias=False)\n",
    "        self.W_v = nn.Linear(dim,self.k,bias=False)\n",
    "        self.W_o = nn.Linear(self.k,dim,bias=False)\n",
    "        #self.W_c = nn.Linear(vocab_size,vocab_size,bias=False)\n",
    "\n",
    "    def scaled_dot_product_attention(self,keys,queries,values):\n",
    "        # Find the product if K and Q transpose and divide by the square root of the model dimension (d_model)\n",
    "\n",
    "        pre_softmax_attention_matrix = torch.einsum('bij,bkj->bik', keys,queries)/np.sqrt(self.d_model)\n",
    "        attention_matrix = torch.softmax(pre_softmax_attention_matrix,dim=-1)\n",
    "        attention_output = torch.einsum( 'bij,bjk->bik' , attention_matrix, values)\n",
    "\n",
    "        return attention_output, attention_matrix\n",
    "\n",
    "    def forward(self,x,c):\n",
    "        norm_x = self.layer_norm(x)\n",
    "        #modified_c = self.W_c(c.transpose(-2,-1)).transpose(-2,-1)\n",
    "        modified_c = c\n",
    "\n",
    "        Q = self.W_k(norm_x)\n",
    "        K = self.W_q(norm_x)\n",
    "        V = self.W_v(norm_x)\n",
    "\n",
    "        attention_output, attention_matrix = self.scaled_dot_product_attention(Q,K,V)\n",
    "\n",
    "        #print(attention_matrix.size(),modified_c.size())\n",
    "\n",
    "        attended_c = torch.einsum('bij,bjk->bik',attention_matrix,modified_c)\n",
    "        \n",
    "        #print(c.size(),attended_c.size())\n",
    "\n",
    "        output_x = self.W_o(attention_output) + x*(1/self.num_heads)\n",
    "        output_c = (attended_c + c)*(1/self.num_heads)\n",
    "\n",
    "        return output_x, output_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    \"\"\"Multi-Head Attention layer for metabolic modeling\"\"\"\n",
    "    def __init__(self,vocab_size=115,dim=6,num_heads=2):\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "\n",
    "        self.attention_blocks = nn.ModuleList([AttentionBlock(vocab_size,dim,num_heads) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self,x,c):\n",
    "\n",
    "        output_x = torch.zeros_like(x)\n",
    "        output_c = torch.zeros_like(c)\n",
    "\n",
    "        for attention_block in self.attention_blocks:\n",
    "            o_x, o_c = attention_block(x,c)\n",
    "            output_x += o_x\n",
    "            output_c += o_c\n",
    "\n",
    "        return output_x, output_c\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model,inner_dim_multiplier,dropout=0.1):\n",
    "        super(FeedForwardBlock, self).__init__()\n",
    "\n",
    "        self.d_model = d_model+1\n",
    "        self.inner_dim = inner_dim_multiplier*(d_model+1)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.d_model)\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(self.d_model,self.inner_dim)\n",
    "        self.linear_layer_2 = nn.Linear(self.inner_dim,self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x,c):\n",
    "\n",
    "        y = torch.cat((x,c),2)\n",
    "        \n",
    "        norm_y = self.layer_norm(y)\n",
    "        \n",
    "        norm_y = self.linear_layer_1(norm_y)\n",
    "        norm_y = F.relu(norm_y)\n",
    "        norm_y = self.linear_layer_2(norm_y)\n",
    "\n",
    "        return norm_y + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Embedding layer + Attention Block + FeedForward Layer\"\"\"\n",
    "    def __init__(self,vocab_size=115,dim=6,num_heads=2,inner_dim_multiplier=5):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.d_model = dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.inp_embedding = nn.Embedding(vocab_size,dim)\n",
    "\n",
    "        self.attention_block = MultiHeadAttentionBlock(vocab_size,dim,num_heads)\n",
    "\n",
    "        self.feedforward_block = FeedForwardBlock(dim,inner_dim_multiplier)\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(vocab_size,vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.32)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.32)\n",
    "    \n",
    "    \n",
    "    def forward(self,c):\n",
    "\n",
    "        batch_size, vocab_size, _ = c.size()\n",
    "\n",
    "        # y = torch.randint(0, vocab_size, (batch_size, vocab_size))\n",
    "        # for k in range(vocab_size):\n",
    "        #     y[:,k] = k\n",
    "\n",
    "        y = torch.arange(vocab_size,device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        x = self.inp_embedding(y)\n",
    "        # print(x.size())\n",
    "        \n",
    "        output_x, output_c = self.attention_block(x,c)\n",
    "\n",
    "        output_y = self.feedforward_block(output_x,output_c)\n",
    "\n",
    "        return output_y[:,:,-1].unsqueeze(-1)\n",
    "\n",
    "        #return output_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersSeries(nn.Module):\n",
    "    def __init__(self,vocab_size=115,dim=6,num_heads=2,inner_dim_multiplier=5,num_transformers=2):\n",
    "        super(TransformersSeries, self).__init__()\n",
    "\n",
    "        self.d_model = dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_transformers = num_transformers\n",
    "\n",
    "        self.inp_embedding = nn.Embedding(vocab_size,dim)\n",
    "\n",
    "        self.attention_blocks = nn.ModuleList([MultiHeadAttentionBlock(vocab_size,dim,num_heads) for _ in range(num_transformers)])\n",
    "\n",
    "        self.feedforward_blocks = nn.ModuleList([FeedForwardBlock(dim,inner_dim_multiplier) for _ in range(num_transformers)])\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(vocab_size,vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    \n",
    "    def forward(self,c):\n",
    "\n",
    "        batch_size, vocab_size, _ = c.size()\n",
    "\n",
    "        y = torch.arange(vocab_size,device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        x = self.inp_embedding(y)\n",
    "\n",
    "        \n",
    "        # The series of transformer layers\n",
    "        for i in range(self.num_transformers):\n",
    "\n",
    "            output_x, output_c = self.attention_blocks[i](x,c)\n",
    "\n",
    "            output_y = self.feedforward_blocks[i](output_x,output_c)\n",
    "\n",
    "            x = output_y[:,:,:-1]\n",
    "            c = output_y[:,:,-1].unsqueeze(-1)\n",
    "\n",
    "        return output_y[:,:,-1].unsqueeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps_test_ = torch.load('inputs_test.pt')\n",
    "outs_test_ = torch.load('outputs_test.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformersSeries(\n",
       "  (inp_embedding): Embedding(115, 6)\n",
       "  (attention_blocks): ModuleList(\n",
       "    (0-1): 2 x MultiHeadAttentionBlock(\n",
       "      (attention_blocks): ModuleList(\n",
       "        (0-1): 2 x AttentionBlock(\n",
       "          (layer_norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "          (W_k): Linear(in_features=6, out_features=3, bias=False)\n",
       "          (W_q): Linear(in_features=6, out_features=3, bias=False)\n",
       "          (W_v): Linear(in_features=6, out_features=3, bias=False)\n",
       "          (W_o): Linear(in_features=3, out_features=6, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feedforward_blocks): ModuleList(\n",
       "    (0-1): 2 x FeedForwardBlock(\n",
       "      (layer_norm): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear_layer_1): Linear(in_features=7, out_features=35, bias=True)\n",
       "      (linear_layer_2): Linear(in_features=35, out_features=7, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (linear_layer_1): Linear(in_features=115, out_features=115, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = TransformersSeries()  # Must recreate model structure\n",
    "trained_model.load_state_dict(torch.load('trained_model.pth'))\n",
    "trained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformersSeries(\n",
       "  (inp_embedding): Embedding(115, 12)\n",
       "  (attention_blocks): ModuleList(\n",
       "    (0-1): 2 x MultiHeadAttentionBlock(\n",
       "      (attention_blocks): ModuleList(\n",
       "        (0-2): 3 x AttentionBlock(\n",
       "          (layer_norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "          (W_k): Linear(in_features=12, out_features=4, bias=False)\n",
       "          (W_q): Linear(in_features=12, out_features=4, bias=False)\n",
       "          (W_v): Linear(in_features=12, out_features=4, bias=False)\n",
       "          (W_o): Linear(in_features=4, out_features=12, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feedforward_blocks): ModuleList(\n",
       "    (0-1): 2 x FeedForwardBlock(\n",
       "      (layer_norm): LayerNorm((13,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear_layer_1): Linear(in_features=13, out_features=65, bias=True)\n",
       "      (linear_layer_2): Linear(in_features=65, out_features=13, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (linear_layer_1): Linear(in_features=115, out_features=115, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model_12_3_5_2 = TransformersSeries(115,12,3,5,2)\n",
    "trained_model_12_3_5_2.load_state_dict(torch.load('trained_model_12_3_5_2.pth'))\n",
    "\n",
    "trained_model_12_3_5_2.to(device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [trained_model,trained_model_12_3_5_2]\n",
    "model_configs = ['dmodel=6,num_heads=2,inner_dim_multiplier=5,num_transformers=2','dmodel=12,num_heads=3,inner_dim_multiplier=5,num_transformers=2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(j,k):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    pred_ops = models[k](inps_test_[j,:,:].unsqueeze(0))\n",
    "    target_ops = outs_test_[j,:,:].unsqueeze(0)\n",
    "    inputs = inps_test_[j,:,:].unsqueeze(0)\n",
    "    plt.stem(inputs[0,:,0].cpu().detach().numpy(),'g')\n",
    "    plt.plot(pred_ops[0,:,0].cpu().detach().numpy(),color='red',label='Predicted Concentrations')\n",
    "    plt.plot(target_ops[0,:,0].cpu().detach().numpy(),color='black',label='Actual Concentrations')\n",
    "    plt.ylim([-50,100])\n",
    "    plt.legend()\n",
    "    plt.title(f\"Simulation {j}:{model_configs[k]}\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a51177c40a1415bb845fa159650c7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=14932, description='j', max=29864), IntSlider(value=0, description='k', …"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactive_plot = interactive(plot_data, j=(0, inps_test_.shape[0] - 1),k=(0,1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29865, 115, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_outs_ = models[0](inps_test_)\n",
    "pred_outs_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_actual(n):\n",
    "\n",
    "    plt.plot(pred_outs_[:,j,:].cpu().detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biocomp-NN-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
