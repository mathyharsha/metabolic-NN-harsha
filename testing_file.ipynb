{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual, interactive_output\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: mps (Mac GPU)\n",
      "PyTorch version: 2.9.0.dev20250715\n",
      "Using Mac Metal Performance Shaders for GPU acceleration\n",
      "PyTorch version: 2.9.0.dev20250715\n",
      "Using Mac Metal Performance Shaders for GPU acceleration\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')  # Mac Metal Performance Shaders\n",
    "    print(f\"Training on device: {device} (Mac GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Training on device: {device} (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Training on device: {device} (CPU)\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if device.type == 'mps':\n",
    "    print(\"Using Mac Metal Performance Shaders for GPU acceleration\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if device.type == 'mps':\n",
    "    print(\"Using Mac Metal Performance Shaders for GPU acceleration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Transformer layer for metabolic modeling\"\"\"\n",
    "    def __init__(self,vocab_size=115,dim=6,num_heads=2):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        assert dim%num_heads==0, \"model dimension must be divisible by number of heads\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = dim\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.k = dim//num_heads\n",
    "\n",
    "        self.W_k = nn.Linear(dim,self.k,bias=False)\n",
    "        self.W_q = nn.Linear(dim,self.k,bias=False)\n",
    "        self.W_v = nn.Linear(dim,self.k,bias=False)\n",
    "        self.W_o = nn.Linear(self.k,dim,bias=False)\n",
    "        #self.W_c = nn.Linear(vocab_size,vocab_size,bias=False)\n",
    "\n",
    "    def scaled_dot_product_attention(self,keys,queries,values):\n",
    "        # Find the product if K and Q transpose and divide by the square root of the model dimension (d_model)\n",
    "\n",
    "        pre_softmax_attention_matrix = torch.einsum('bij,bkj->bik', keys,queries)/np.sqrt(self.d_model)\n",
    "        attention_matrix = torch.softmax(pre_softmax_attention_matrix,dim=-1)\n",
    "        attention_output = torch.einsum( 'bij,bjk->bik' , attention_matrix, values)\n",
    "\n",
    "        return attention_output, attention_matrix\n",
    "\n",
    "    def forward(self,x,c):\n",
    "        norm_x = self.layer_norm(x)\n",
    "        #modified_c = self.W_c(c.transpose(-2,-1)).transpose(-2,-1)\n",
    "        modified_c = c\n",
    "\n",
    "        Q = self.W_k(norm_x)\n",
    "        K = self.W_q(norm_x)\n",
    "        V = self.W_v(norm_x)\n",
    "\n",
    "        attention_output, attention_matrix = self.scaled_dot_product_attention(Q,K,V)\n",
    "\n",
    "        #print(attention_matrix.size(),modified_c.size())\n",
    "\n",
    "        attended_c = torch.einsum('bij,bjk->bik',attention_matrix,modified_c)\n",
    "        \n",
    "        #print(c.size(),attended_c.size())\n",
    "\n",
    "        output_x = self.W_o(attention_output) + x*(1/self.num_heads)\n",
    "        output_c = (attended_c + c)*(1/self.num_heads)\n",
    "\n",
    "        return output_x, output_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    \"\"\"Multi-Head Attention layer for metabolic modeling\"\"\"\n",
    "    def __init__(self,vocab_size=115,dim=6,num_heads=2):\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "\n",
    "        self.attention_blocks = nn.ModuleList([AttentionBlock(vocab_size,dim,num_heads) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self,x,c):\n",
    "\n",
    "        output_x = torch.zeros_like(x)\n",
    "        output_c = torch.zeros_like(c)\n",
    "\n",
    "        for attention_block in self.attention_blocks:\n",
    "            o_x, o_c = attention_block(x,c)\n",
    "            output_x += o_x\n",
    "            output_c += o_c\n",
    "\n",
    "        return output_x, output_c\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model,inner_dim_multiplier,dropout=0.1):\n",
    "        super(FeedForwardBlock, self).__init__()\n",
    "\n",
    "        self.d_model = d_model+1\n",
    "        self.inner_dim = inner_dim_multiplier*(d_model+1)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.d_model)\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(self.d_model,self.inner_dim)\n",
    "        self.linear_layer_2 = nn.Linear(self.inner_dim,self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x,c):\n",
    "\n",
    "        y = torch.cat((x,c),2)\n",
    "        \n",
    "        norm_y = self.layer_norm(y)\n",
    "        \n",
    "        norm_y = self.linear_layer_1(norm_y)\n",
    "        norm_y = F.relu(norm_y)\n",
    "        norm_y = self.linear_layer_2(norm_y)\n",
    "\n",
    "        return norm_y + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Embedding layer + Attention Block + FeedForward Layer\"\"\"\n",
    "    def __init__(self,vocab_size=115,dim=6,num_heads=2,inner_dim_multiplier=5):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.d_model = dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.inp_embedding = nn.Embedding(vocab_size,dim)\n",
    "\n",
    "        self.attention_block = MultiHeadAttentionBlock(vocab_size,dim,num_heads)\n",
    "\n",
    "        self.feedforward_block = FeedForwardBlock(dim,inner_dim_multiplier)\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(vocab_size,vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.32)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.32)\n",
    "    \n",
    "    \n",
    "    def forward(self,c):\n",
    "\n",
    "        batch_size, vocab_size, _ = c.size()\n",
    "\n",
    "        # y = torch.randint(0, vocab_size, (batch_size, vocab_size))\n",
    "        # for k in range(vocab_size):\n",
    "        #     y[:,k] = k\n",
    "\n",
    "        y = torch.arange(vocab_size,device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        x = self.inp_embedding(y)\n",
    "        # print(x.size())\n",
    "        \n",
    "        output_x, output_c = self.attention_block(x,c)\n",
    "\n",
    "        output_y = self.feedforward_block(output_x,output_c)\n",
    "\n",
    "        return output_y[:,:,-1].unsqueeze(-1)\n",
    "\n",
    "        #return output_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersSeries(nn.Module):\n",
    "    def __init__(self,vocab_size=115,dim=6,num_heads=2,inner_dim_multiplier=5,num_transformers=2):\n",
    "        super(TransformersSeries, self).__init__()\n",
    "\n",
    "        self.d_model = dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_transformers = num_transformers\n",
    "\n",
    "        self.inp_embedding = nn.Embedding(vocab_size,dim)\n",
    "\n",
    "        self.attention_blocks = nn.ModuleList([MultiHeadAttentionBlock(vocab_size,dim,num_heads) for _ in range(num_transformers)])\n",
    "\n",
    "        self.feedforward_blocks = nn.ModuleList([FeedForwardBlock(dim,inner_dim_multiplier) for _ in range(num_transformers)])\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(vocab_size,vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    \n",
    "    def forward(self,c):\n",
    "\n",
    "        batch_size, vocab_size, _ = c.size()\n",
    "\n",
    "        y = torch.arange(vocab_size,device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        x = self.inp_embedding(y)\n",
    "\n",
    "        \n",
    "        # The series of transformer layers\n",
    "        for i in range(self.num_transformers):\n",
    "\n",
    "            output_x, output_c = self.attention_blocks[i](x,c)\n",
    "\n",
    "            output_y = self.feedforward_blocks[i](output_x,output_c)\n",
    "\n",
    "            x = output_y[:,:,:-1]\n",
    "            c = output_y[:,:,-1].unsqueeze(-1)\n",
    "\n",
    "        return output_y[:,:,-1].unsqueeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps_test_ = torch.load('inputs_test.pt')\n",
    "outs_test_ = torch.load('outputs_test.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [\n",
    "        'EX_glc__D_e', 'EX_fru_e', 'EX_lac__D_e', 'EX_pyr_e', 'EX_ac_e',\n",
    "        'EX_akg_e', 'EX_succ_e', 'EX_fum_e', 'EX_mal__L_e', 'EX_etoh_e',\n",
    "        'EX_acald_e', 'EX_for_e', 'EX_gln__L_e', 'EX_glu__L_e',\n",
    "        'EX_co2_e', 'EX_h_e', 'EX_h2o_e', 'EX_nh4_e', 'EX_o2_e', 'EX_pi_e'\n",
    "    ]\n",
    "\n",
    "output_cols = [\n",
    "        'ACALD_flux',\n",
    "        'ACALDt_flux',\n",
    "        'ACKr_flux',\n",
    "        'ACONTa_flux',\n",
    "        'ACONTb_flux',\n",
    "        'ACt2r_flux',\n",
    "        'ADK1_flux',\n",
    "        'AKGDH_flux',\n",
    "        'AKGt2r_flux',\n",
    "        'ALCD2x_flux',\n",
    "        'ATPM_flux',\n",
    "        'ATPS4r_flux',\n",
    "        'Biomass_Ecoli_core_flux',\n",
    "        'CO2t_flux',\n",
    "        'CS_flux',\n",
    "        'CYTBD_flux',\n",
    "        'D_LACt2_flux',\n",
    "        'ENO_flux',\n",
    "        'ETOHt2r_flux',\n",
    "        'EX_ac_e_flux',\n",
    "        'EX_acald_e_flux',\n",
    "        'EX_akg_e_flux',\n",
    "        'EX_co2_e_flux',\n",
    "        'EX_etoh_e_flux',\n",
    "        'EX_for_e_flux',\n",
    "        'EX_fru_e_flux',\n",
    "        'EX_fum_e_flux',\n",
    "        'EX_glc__D_e_flux',\n",
    "        'EX_gln__L_e_flux',\n",
    "        'EX_glu__L_e_flux',\n",
    "        'EX_h_e_flux',\n",
    "        'EX_h2o_e_flux',\n",
    "        'EX_lac__D_e_flux',\n",
    "        'EX_mal__L_e_flux',\n",
    "        'EX_nh4_e_flux',\n",
    "        'EX_o2_e_flux',\n",
    "        'EX_pi_e_flux',\n",
    "        'EX_pyr_e_flux',\n",
    "        'EX_succ_e_flux',\n",
    "        'FBA_flux',\n",
    "        'FBP_flux',\n",
    "        'FORt2_flux',\n",
    "        'FORti_flux',\n",
    "        'FRD7_flux',\n",
    "        'FRUpts2_flux',\n",
    "        'FUM_flux',\n",
    "        'FUMt2_2_flux',\n",
    "        'G6PDH2r_flux',\n",
    "        'GAPD_flux',\n",
    "        'GLCpts_flux',\n",
    "        'GLNS_flux',\n",
    "        'GLNabc_flux',\n",
    "        'GLUDy_flux',\n",
    "        'GLUN_flux',\n",
    "        'GLUSy_flux',\n",
    "        'GLUt2r_flux',\n",
    "        'GND_flux',\n",
    "        'H2Ot_flux',\n",
    "        'ICDHyr_flux',\n",
    "        'ICL_flux',\n",
    "        'LDH_D_flux',\n",
    "        'MALS_flux',\n",
    "        'MALt2_2_flux',\n",
    "        'MDH_flux',\n",
    "        'ME1_flux',\n",
    "        'ME2_flux',\n",
    "        'NADH16_flux',\n",
    "        'NADTRHD_flux',\n",
    "        'NH4t_flux',\n",
    "        'O2t_flux',\n",
    "        'PDH_flux',\n",
    "        'PFK_flux',\n",
    "        'PFL_flux',\n",
    "        'PGI_flux',\n",
    "        'PGK_flux',\n",
    "        'PGL_flux',\n",
    "        'PGM_flux',\n",
    "        'PIt2r_flux',\n",
    "        'PPC_flux',\n",
    "        'PPCK_flux',\n",
    "        'PPS_flux',\n",
    "        'PTAr_flux',\n",
    "        'PYK_flux',\n",
    "        'PYRt2_flux',\n",
    "        'RPE_flux',\n",
    "        'RPI_flux',\n",
    "        'SUCCt2_2_flux',\n",
    "        'SUCCt3_flux',\n",
    "        'SUCDi_flux',\n",
    "        'SUCOAS_flux',\n",
    "        'TALA_flux',\n",
    "        'THD2_flux',\n",
    "        'TKT1_flux',\n",
    "        'TKT2_flux',\n",
    "        'TPI_flux'\n",
    "    ]\n",
    "\n",
    "all_data = input_cols + output_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = TransformersSeries()  # Must recreate model structure\n",
    "trained_model.load_state_dict(torch.load('trained_model.pth'))\n",
    "trained_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_12_3_5_2 = TransformersSeries(115,12,3,5,2)\n",
    "trained_model_12_3_5_2.load_state_dict(torch.load('trained_model_12_3_5_2.pth'))\n",
    "\n",
    "trained_model_12_3_5_2.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [trained_model,trained_model_12_3_5_2]\n",
    "model_configs = ['dmodel=6,num_heads=2,inner_dim_multiplier=5,num_transformers=2','dmodel=12,num_heads=3,inner_dim_multiplier=5,num_transformers=2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(simulation,model):\n",
    "    plt.figure(figsize=(6*2, 4*2))\n",
    "    j = simulation\n",
    "    k = model\n",
    "    pred_ops = models[k](inps_test_[j,:,:].unsqueeze(0))\n",
    "    target_ops = outs_test_[j,:,:].unsqueeze(0)\n",
    "    inputs = inps_test_[j,:,:].unsqueeze(0)\n",
    "    plt.stem(inputs[0,:,0].cpu().detach().numpy(),'g',label='Input Concentrations')\n",
    "    plt.plot(pred_ops[0,:,0].cpu().detach().numpy(),color='red',label='Predicted Concentrations')\n",
    "    plt.plot(target_ops[0,:,0].cpu().detach().numpy(),color='black',label='Actual Concentrations')\n",
    "    plt.ylim([-50,100])\n",
    "    plt.legend()\n",
    "    plt.title(f\"Simulation {j}:{model_configs[k]}\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511c32ba5733439294c709dc6e0dafa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=14932, description='simulation', max=29864), IntSlider(value=0, descript…"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactive_plot = interactive(plot_data, simulation=(0, inps_test_.shape[0] - 1),model=(0,1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def plot_data_interact(simulation,model):\n",
    "#     j = simulation \n",
    "#     xx = np.array([i for i in range(20)])\n",
    "\n",
    "\n",
    "#     # Create plotly figure\n",
    "#     fig_plotly = go.Figure()\n",
    "#     inputs = inps_test_[j,:20,:].unsqueeze(0)\n",
    "#     inp = inputs[0,:,0].cpu().detach().numpy()\n",
    "\n",
    "#     pred_ops = models[model](inps_test_[j,:,:].unsqueeze(0))\n",
    "#     target_ops = outs_test_[j,:,:].unsqueeze(0)\n",
    "\n",
    "#     fig_plotly.add_trace(go.Scatter(\n",
    "#         x=xx,\n",
    "#         y=inp,\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=12,\n",
    "#             color='green',\n",
    "#         ),\n",
    "#         text=[f'{all_data[i]}' \n",
    "#                 for i in xx],\n",
    "#         hovertemplate='%{text}<extra></extra>',\n",
    "#         name='Input Concentrations'\n",
    "#     ))\n",
    "\n",
    "#     xxx = np.array([i for i in range(115)])\n",
    "\n",
    "#     fig_plotly.add_trace(go.Scatter(\n",
    "#         x=xxx, \n",
    "#         y = pred_ops[0,:,0].cpu().detach().numpy(), \n",
    "#         mode='lines+markers', \n",
    "#         text = [f'{all_data[i]}' \n",
    "#                 for i in xxx],\n",
    "#         hovertemplate='%{text}<extra></extra>',\n",
    "#         name='Predicted Concentrations'\n",
    "#         ))\n",
    "\n",
    "#     fig_plotly.add_trace(go.Scatter(\n",
    "#         x=xxx, \n",
    "#         y = target_ops[0,:,0].cpu().detach().numpy(), \n",
    "#         mode='lines+markers', \n",
    "#         text = [f'{all_data[i]}' \n",
    "#                 for i in xxx],\n",
    "#         hovertemplate='%{text}<extra></extra>',\n",
    "#         name='Predicted Concentrations'\n",
    "#         ))\n",
    "\n",
    "#     fig_plotly.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive_plot = interactive(plot_data_interact, simulation=(0, inps_test_.shape[0] - 1),model=(0,1))\n",
    "# output = interactive_plot.children[-1]\n",
    "# display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62226fbc0cf24e56b0468b740393ff28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Simulation:', max=29864), Dropdown(description='Model:',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec574f182da498f8a8a4dbce7eadb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'hovertemplate': '%{text}<extra></extra>',\n",
       "              'marker': {'color': 'purple', 'size': 12},\n",
       "              'mode': 'markers',\n",
       "              'name': 'Input Concentrations',\n",
       "              'text': [EX_glc__D_e, EX_fru_e, EX_lac__D_e, EX_pyr_e, EX_ac_e,\n",
       "                       EX_akg_e, EX_succ_e, EX_fum_e, EX_mal__L_e, EX_etoh_e,\n",
       "                       EX_acald_e, EX_for_e, EX_gln__L_e, EX_glu__L_e, EX_co2_e,\n",
       "                       EX_h_e, EX_h2o_e, EX_nh4_e, EX_o2_e, EX_pi_e],\n",
       "              'type': 'scatter',\n",
       "              'uid': '7902272a-d2e0-4e35-8ad1-603028d88fa9',\n",
       "              'x': {'bdata': 'AAECAwQFBgcICQoLDA0ODxAREhM=', 'dtype': 'i1'},\n",
       "              'y': {'bdata': ('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA' ... 'hCAABIQgAASEK4HiVBZmYxQkjh6kA='),\n",
       "                    'dtype': 'f4'}},\n",
       "             {'hovertemplate': '%{text}<extra></extra>',\n",
       "              'mode': 'lines+markers',\n",
       "              'name': 'Predicted Concentrations',\n",
       "              'text': [EX_glc__D_e, EX_fru_e, EX_lac__D_e, EX_pyr_e, EX_ac_e,\n",
       "                       EX_akg_e, EX_succ_e, EX_fum_e, EX_mal__L_e, EX_etoh_e,\n",
       "                       EX_acald_e, EX_for_e, EX_gln__L_e, EX_glu__L_e, EX_co2_e,\n",
       "                       EX_h_e, EX_h2o_e, EX_nh4_e, EX_o2_e, EX_pi_e, ACALD_flux,\n",
       "                       ACALDt_flux, ACKr_flux, ACONTa_flux, ACONTb_flux,\n",
       "                       ACt2r_flux, ADK1_flux, AKGDH_flux, AKGt2r_flux, ALCD2x_flux,\n",
       "                       ATPM_flux, ATPS4r_flux, Biomass_Ecoli_core_flux, CO2t_flux,\n",
       "                       CS_flux, CYTBD_flux, D_LACt2_flux, ENO_flux, ETOHt2r_flux,\n",
       "                       EX_ac_e_flux, EX_acald_e_flux, EX_akg_e_flux, EX_co2_e_flux,\n",
       "                       EX_etoh_e_flux, EX_for_e_flux, EX_fru_e_flux, EX_fum_e_flux,\n",
       "                       EX_glc__D_e_flux, EX_gln__L_e_flux, EX_glu__L_e_flux,\n",
       "                       EX_h_e_flux, EX_h2o_e_flux, EX_lac__D_e_flux,\n",
       "                       EX_mal__L_e_flux, EX_nh4_e_flux, EX_o2_e_flux, EX_pi_e_flux,\n",
       "                       EX_pyr_e_flux, EX_succ_e_flux, FBA_flux, FBP_flux,\n",
       "                       FORt2_flux, FORti_flux, FRD7_flux, FRUpts2_flux, FUM_flux,\n",
       "                       FUMt2_2_flux, G6PDH2r_flux, GAPD_flux, GLCpts_flux,\n",
       "                       GLNS_flux, GLNabc_flux, GLUDy_flux, GLUN_flux, GLUSy_flux,\n",
       "                       GLUt2r_flux, GND_flux, H2Ot_flux, ICDHyr_flux, ICL_flux,\n",
       "                       LDH_D_flux, MALS_flux, MALt2_2_flux, MDH_flux, ME1_flux,\n",
       "                       ME2_flux, NADH16_flux, NADTRHD_flux, NH4t_flux, O2t_flux,\n",
       "                       PDH_flux, PFK_flux, PFL_flux, PGI_flux, PGK_flux, PGL_flux,\n",
       "                       PGM_flux, PIt2r_flux, PPC_flux, PPCK_flux, PPS_flux,\n",
       "                       PTAr_flux, PYK_flux, PYRt2_flux, RPE_flux, RPI_flux,\n",
       "                       SUCCt2_2_flux, SUCCt3_flux, SUCDi_flux, SUCOAS_flux,\n",
       "                       TALA_flux, THD2_flux, TKT1_flux, TKT2_flux, TPI_flux],\n",
       "              'type': 'scatter',\n",
       "              'uid': 'f545feb6-7ff3-4028-b567-baf3283fff56',\n",
       "              'x': {'bdata': ('AAECAwQFBgcICQoLDA0ODxAREhMUFR' ... '5fYGFiY2RlZmdoaWprbG1ub3Bxcg=='),\n",
       "                    'dtype': 'i1'},\n",
       "              'y': {'bdata': ('Oi6Lv2grJb8AWFG9oLXfPsCjuz3gQj' ... 'AUXD2/6MAGvyRePb+kpXG/QE5lvw=='),\n",
       "                    'dtype': 'f4'}},\n",
       "             {'hovertemplate': '%{text}<extra></extra>',\n",
       "              'mode': 'lines+markers',\n",
       "              'name': 'Target Concentrations',\n",
       "              'text': [EX_glc__D_e, EX_fru_e, EX_lac__D_e, EX_pyr_e, EX_ac_e,\n",
       "                       EX_akg_e, EX_succ_e, EX_fum_e, EX_mal__L_e, EX_etoh_e,\n",
       "                       EX_acald_e, EX_for_e, EX_gln__L_e, EX_glu__L_e, EX_co2_e,\n",
       "                       EX_h_e, EX_h2o_e, EX_nh4_e, EX_o2_e, EX_pi_e, ACALD_flux,\n",
       "                       ACALDt_flux, ACKr_flux, ACONTa_flux, ACONTb_flux,\n",
       "                       ACt2r_flux, ADK1_flux, AKGDH_flux, AKGt2r_flux, ALCD2x_flux,\n",
       "                       ATPM_flux, ATPS4r_flux, Biomass_Ecoli_core_flux, CO2t_flux,\n",
       "                       CS_flux, CYTBD_flux, D_LACt2_flux, ENO_flux, ETOHt2r_flux,\n",
       "                       EX_ac_e_flux, EX_acald_e_flux, EX_akg_e_flux, EX_co2_e_flux,\n",
       "                       EX_etoh_e_flux, EX_for_e_flux, EX_fru_e_flux, EX_fum_e_flux,\n",
       "                       EX_glc__D_e_flux, EX_gln__L_e_flux, EX_glu__L_e_flux,\n",
       "                       EX_h_e_flux, EX_h2o_e_flux, EX_lac__D_e_flux,\n",
       "                       EX_mal__L_e_flux, EX_nh4_e_flux, EX_o2_e_flux, EX_pi_e_flux,\n",
       "                       EX_pyr_e_flux, EX_succ_e_flux, FBA_flux, FBP_flux,\n",
       "                       FORt2_flux, FORti_flux, FRD7_flux, FRUpts2_flux, FUM_flux,\n",
       "                       FUMt2_2_flux, G6PDH2r_flux, GAPD_flux, GLCpts_flux,\n",
       "                       GLNS_flux, GLNabc_flux, GLUDy_flux, GLUN_flux, GLUSy_flux,\n",
       "                       GLUt2r_flux, GND_flux, H2Ot_flux, ICDHyr_flux, ICL_flux,\n",
       "                       LDH_D_flux, MALS_flux, MALt2_2_flux, MDH_flux, ME1_flux,\n",
       "                       ME2_flux, NADH16_flux, NADTRHD_flux, NH4t_flux, O2t_flux,\n",
       "                       PDH_flux, PFK_flux, PFL_flux, PGI_flux, PGK_flux, PGL_flux,\n",
       "                       PGM_flux, PIt2r_flux, PPC_flux, PPCK_flux, PPS_flux,\n",
       "                       PTAr_flux, PYK_flux, PYRt2_flux, RPE_flux, RPI_flux,\n",
       "                       SUCCt2_2_flux, SUCCt3_flux, SUCDi_flux, SUCOAS_flux,\n",
       "                       TALA_flux, THD2_flux, TKT1_flux, TKT2_flux, TPI_flux],\n",
       "              'type': 'scatter',\n",
       "              'uid': 'dd9a3120-8c26-4dce-8a70-eef1b8e7fa14',\n",
       "              'x': {'bdata': ('AAECAwQFBgcICQoLDA0ODxAREhMUFR' ... '5fYGFiY2RlZmdoaWprbG1ub3Bxcg=='),\n",
       "                    'dtype': 'i1'},\n",
       "              'y': {'bdata': ('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAA' ... 'BHoeS9AAAAAEeh5L2wfqy+gOYevw=='),\n",
       "                    'dtype': 'f4'}}],\n",
       "    'layout': {'height': 800,\n",
       "               'legend': {'orientation': 'h', 'x': 0.5, 'xanchor': 'center', 'y': 1.02, 'yanchor': 'bottom'},\n",
       "               'template': '...',\n",
       "               'title': {'text': 'Simulation 0 - Model 0'},\n",
       "               'width': 1700,\n",
       "               'xaxis': {'dtick': 10, 'tick0': 0, 'tickmode': 'linear', 'title': {'text': 'Time Steps'}},\n",
       "               'yaxis': {'range': [-50, 100], 'title': {'text': 'Concentrations'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Create FigureWidget with increased size\n",
    "fig_widget = go.FigureWidget(\n",
    "    layout=go.Layout(\n",
    "        width=1700,   # Increased width\n",
    "        height=800,   # Increased height\n",
    "        title='Interactive Plot',\n",
    "        yaxis=dict(range=[-50, 100]),\n",
    "        xaxis=dict(\n",
    "            tickmode='linear',\n",
    "            tick0=0,\n",
    "            dtick=10  # Tick every 10 units for larger spacing\n",
    "        ),\n",
    "        legend=dict(\n",
    "            orientation=\"h\",  # Horizontal orientation\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,          # Position above the plot\n",
    "            xanchor=\"center\",\n",
    "            x=0.5            # Center horizontally\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "def update_plot(simulation, model):\n",
    "    j = simulation\n",
    "    xx = np.array([i for i in range(20)])\n",
    "    \n",
    "    inputs = inps_test_[j, :20, :].unsqueeze(0)\n",
    "    inp = inputs[0, :, 0].cpu().detach().numpy()\n",
    "    pred_ops = models[model](inps_test_[j, :, :].unsqueeze(0))\n",
    "    target_ops = outs_test_[j, :, :].unsqueeze(0)\n",
    "    \n",
    "    xxx = np.array([i for i in range(115)])\n",
    "    \n",
    "    # Clear existing traces\n",
    "    fig_widget.data = []\n",
    "    \n",
    "    # Add traces\n",
    "    fig_widget.add_trace(go.Scatter(\n",
    "        x=xx,\n",
    "        y=inp,\n",
    "        mode='markers',\n",
    "        marker=dict(size=12, color='purple'),\n",
    "        text=[f'{all_data[i]}' for i in xx],\n",
    "        hovertemplate='%{text}<extra></extra>',\n",
    "        name='Input Concentrations'\n",
    "    ))\n",
    "    \n",
    "    fig_widget.add_trace(go.Scatter(\n",
    "        x=xxx,\n",
    "        y=pred_ops[0, :, 0].cpu().detach().numpy(),\n",
    "        mode='lines+markers',\n",
    "        text=[f'{all_data[i]}' for i in xxx],\n",
    "        hovertemplate='%{text}<extra></extra>',\n",
    "        name='Predicted Concentrations'\n",
    "    ))\n",
    "    \n",
    "    fig_widget.add_trace(go.Scatter(\n",
    "        x=xxx,\n",
    "        y=target_ops[0, :, 0].cpu().detach().numpy(),\n",
    "        mode='lines+markers',\n",
    "        text=[f'{all_data[i]}' for i in xxx],\n",
    "        hovertemplate='%{text}<extra></extra>',\n",
    "        name='Target Concentrations'\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig_widget.update_layout(\n",
    "        title=f'Simulation {simulation} - Model {model}',\n",
    "        xaxis_title='Time Steps',\n",
    "        yaxis_title='Concentrations'\n",
    "    )\n",
    "\n",
    "# Create interactive widget\n",
    "interactive_plot = widgets.interactive(\n",
    "    update_plot,\n",
    "    simulation=widgets.IntSlider(\n",
    "        min=0, \n",
    "        max=len(inps_test_) - 1,\n",
    "        step=1, \n",
    "        value=0,\n",
    "        description='Simulation:'\n",
    "    ),\n",
    "    model=widgets.Dropdown(\n",
    "        options=list(range(len(models))),\n",
    "        value=0,\n",
    "        description='Model:'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display both the controls and the figure\n",
    "display(interactive_plot, fig_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29865, 115, 1])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_outs_ = models[0](inps_test_)\n",
    "pred_outs_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29865, 115, 1])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs_test_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_actual(metabolite,simulation,model):  \n",
    "    pred_outs_ = models[model](inps_test_)\n",
    "    plt.scatter(pred_outs_[:,metabolite,:].cpu().detach().numpy(),outs_test_[:,metabolite,:].cpu().detach().numpy(),color='r',alpha=0.1)\n",
    "    plt.scatter(pred_outs_[simulation,metabolite,:].cpu().detach().numpy(),outs_test_[simulation,metabolite,:].cpu().detach().numpy(),color='k',alpha=0.9)\n",
    "    plt.xlim([-50,100])\n",
    "    plt.ylim([-50,100])\n",
    "    plt.grid()\n",
    "    plt.xlabel('Predicted Output')\n",
    "    plt.ylabel('Actual Output')\n",
    "    plt.title('Input Output Characterstics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80258f697864c1a9656071e1e97169c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=57, description='metabolite', max=114), IntSlider(value=14932, descripti…"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactive_plot = interactive(plot_pred_actual, metabolite=(0, inps_test_.shape[1] - 1), simulation = (0, inps_test_.shape[0] - 1), model = (0,1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio-nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
