{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')  # Mac Metal Performance Shaders\n",
    "    print(f\"Training on device: {device} (Mac GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Training on device: {device} (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Training on device: {device} (CPU)\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if device.type == 'mps':\n",
    "    print(\"Using Mac Metal Performance Shaders for GPU acceleration\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if device.type == 'mps':\n",
    "    print(\"Using Mac Metal Performance Shaders for GPU acceleration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Transformer layer for metabolic modeling\"\"\"\n",
    "    def __init__(self,vocab_size=115,dim=6,num_heads=2):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        assert dim%num_heads==0, \"model dimension must be divisible by number of heads\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = dim\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.k = dim//num_heads\n",
    "\n",
    "        self.W_k = nn.Linear(dim,self.k,bias=False)\n",
    "        self.W_q = nn.Linear(dim,self.k,bias=False)\n",
    "        self.W_v = nn.Linear(dim,self.k,bias=False)\n",
    "        self.W_o = nn.Linear(self.k,dim,bias=False)\n",
    "        #self.W_c = nn.Linear(vocab_size,vocab_size,bias=False)\n",
    "\n",
    "    def scaled_dot_product_attention(self,keys,queries,values):\n",
    "        # Find the product if K and Q transpose and divide by the square root of the model dimension (d_model)\n",
    "\n",
    "        pre_softmax_attention_matrix = torch.einsum('bij,bkj->bik', keys,queries)/np.sqrt(self.d_model)\n",
    "        attention_matrix = torch.softmax(pre_softmax_attention_matrix,dim=-1)\n",
    "        attention_output = torch.einsum( 'bij,bjk->bik' , attention_matrix, values)\n",
    "\n",
    "        return attention_output, attention_matrix\n",
    "\n",
    "    def forward(self,x,c):\n",
    "        norm_x = self.layer_norm(x)\n",
    "        #modified_c = self.W_c(c.transpose(-2,-1)).transpose(-2,-1)\n",
    "        modified_c = c\n",
    "\n",
    "        Q = self.W_k(norm_x)\n",
    "        K = self.W_q(norm_x)\n",
    "        V = self.W_v(norm_x)\n",
    "\n",
    "        attention_output, attention_matrix = self.scaled_dot_product_attention(Q,K,V)\n",
    "\n",
    "        #print(attention_matrix.size(),modified_c.size())\n",
    "\n",
    "        attended_c = torch.einsum('bij,bjk->bik',attention_matrix,modified_c)\n",
    "        \n",
    "        #print(c.size(),attended_c.size())\n",
    "\n",
    "        output_x = self.W_o(attention_output) + x*(1/self.num_heads)\n",
    "        output_c = (attended_c + c)*(1/self.num_heads)\n",
    "\n",
    "        return output_x, output_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    \"\"\"Multi-Head Attention layer for metabolic modeling\"\"\"\n",
    "    def __init__(self,vocab_size=115,dim=6,num_heads=2):\n",
    "        super(MultiHeadAttentionBlock, self).__init__()\n",
    "\n",
    "        self.attention_blocks = nn.ModuleList([AttentionBlock(vocab_size,dim,num_heads) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self,x,c):\n",
    "\n",
    "        output_x = torch.zeros_like(x)\n",
    "        output_c = torch.zeros_like(c)\n",
    "\n",
    "        for attention_block in self.attention_blocks:\n",
    "            o_x, o_c = attention_block(x,c)\n",
    "            output_x += o_x\n",
    "            output_c += o_c\n",
    "\n",
    "        return output_x, output_c\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model,inner_dim_multiplier,dropout=0.1):\n",
    "        super(FeedForwardBlock, self).__init__()\n",
    "\n",
    "        self.d_model = d_model+1\n",
    "        self.inner_dim = inner_dim_multiplier*(d_model+1)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.d_model)\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(self.d_model,self.inner_dim)\n",
    "        self.linear_layer_2 = nn.Linear(self.inner_dim,self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x,c):\n",
    "\n",
    "        y = torch.cat((x,c),2)\n",
    "        \n",
    "        norm_y = self.layer_norm(y)\n",
    "        \n",
    "        norm_y = self.linear_layer_1(norm_y)\n",
    "        norm_y = F.relu(norm_y)\n",
    "        norm_y = self.linear_layer_2(norm_y)\n",
    "\n",
    "        return norm_y + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Embedding layer + Attention Block + FeedForward Layer\"\"\"\n",
    "    def __init__(self,vocab_size=115,dim=6,num_heads=2,inner_dim_multiplier=5):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.d_model = dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.inp_embedding = nn.Embedding(vocab_size,dim)\n",
    "\n",
    "        self.attention_block = MultiHeadAttentionBlock(vocab_size,dim,num_heads)\n",
    "\n",
    "        self.feedforward_block = FeedForwardBlock(dim,inner_dim_multiplier)\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(vocab_size,vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.32)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.32)\n",
    "    \n",
    "    \n",
    "    def forward(self,c):\n",
    "\n",
    "        batch_size, vocab_size, _ = c.size()\n",
    "\n",
    "        # y = torch.randint(0, vocab_size, (batch_size, vocab_size))\n",
    "        # for k in range(vocab_size):\n",
    "        #     y[:,k] = k\n",
    "\n",
    "        y = torch.arange(vocab_size,device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        x = self.inp_embedding(y)\n",
    "        # print(x.size())\n",
    "        \n",
    "        output_x, output_c = self.attention_block(x,c)\n",
    "\n",
    "        output_y = self.feedforward_block(output_x,output_c)\n",
    "\n",
    "        return output_y[:,:,-1].unsqueeze(-1)\n",
    "\n",
    "        #return output_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersSeries(nn.Module):\n",
    "    def __init__(self,vocab_size=115,dim=6,num_heads=2,inner_dim_multiplier=5,num_transformers=2):\n",
    "        super(TransformersSeries, self).__init__()\n",
    "\n",
    "        self.d_model = dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_transformers = num_transformers\n",
    "\n",
    "        self.inp_embedding = nn.Embedding(vocab_size,dim)\n",
    "\n",
    "        self.attention_blocks = nn.ModuleList([MultiHeadAttentionBlock(vocab_size,dim,num_heads) for _ in range(num_transformers)])\n",
    "\n",
    "        self.feedforward_blocks = nn.ModuleList([FeedForwardBlock(dim,inner_dim_multiplier) for _ in range(num_transformers)])\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(vocab_size,vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    \n",
    "    def forward(self,c):\n",
    "\n",
    "        batch_size, vocab_size, _ = c.size()\n",
    "\n",
    "        y = torch.arange(vocab_size,device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        x = self.inp_embedding(y)\n",
    "\n",
    "        \n",
    "        # The series of transformer layers\n",
    "        for i in range(self.num_transformers):\n",
    "\n",
    "            output_x, output_c = self.attention_blocks[i](x,c)\n",
    "\n",
    "            output_y = self.feedforward_blocks[i](output_x,output_c)\n",
    "\n",
    "            x = output_y[:,:,:-1]\n",
    "            c = output_y[:,:,-1].unsqueeze(-1)\n",
    "\n",
    "        return output_y[:,:,-1].unsqueeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inps_test_ = torch.load('inputs_test.pt')\n",
    "outs_test_ = torch.load('outputs_test.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = TransformersSeries()  # Must recreate model structure\n",
    "trained_model.load_state_dict(torch.load('trained_model.pth'))\n",
    "trained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_12_3_5_2 = TransformersSeries(115,12,3,5,2)\n",
    "trained_model_12_3_5_2.load_state_dict(torch.load('trained_model_12_3_5_2.pth'))\n",
    "\n",
    "trained_model_12_3_5_2.to(device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [trained_model,trained_model_12_3_5_2]\n",
    "model_configs = ['dmodel=6,num_heads=2,inner_dim_multiplier=5,num_transformers=2','dmodel=12,num_heads=3,inner_dim_multiplier=5,num_transformers=2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(simulation,model):\n",
    "    #plt.figure(figsize=(6, 4))\n",
    "    j = simulation\n",
    "    k = model\n",
    "    pred_ops = models[k](inps_test_[j,:,:].unsqueeze(0))\n",
    "    target_ops = outs_test_[j,:,:].unsqueeze(0)\n",
    "    inputs = inps_test_[j,:,:].unsqueeze(0)\n",
    "    plt.stem(inputs[0,:,0].cpu().detach().numpy(),'g',label='Input Concentrations')\n",
    "    plt.plot(pred_ops[0,:,0].cpu().detach().numpy(),color='red',label='Predicted Concentrations')\n",
    "    plt.plot(target_ops[0,:,0].cpu().detach().numpy(),color='black',label='Actual Concentrations')\n",
    "    plt.ylim([-50,100])\n",
    "    plt.legend()\n",
    "    plt.title(f\"Simulation {j}:{model_configs[k]}\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(plot_data, simulation=(0, inps_test_.shape[0] - 1),model=(0,1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_outs_ = models[0](inps_test_)\n",
    "pred_outs_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs_test_.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_actual(metabolite,simulation,model):  \n",
    "    pred_outs_ = models[model](inps_test_)\n",
    "    plt.scatter(pred_outs_[:,metabolite,:].cpu().detach().numpy(),outs_test_[:,metabolite,:].cpu().detach().numpy(),color='r',alpha=0.1)\n",
    "    plt.scatter(pred_outs_[simulation,metabolite,:].cpu().detach().numpy(),outs_test_[simulation,metabolite,:].cpu().detach().numpy(),color='k',alpha=0.9)\n",
    "    plt.xlim([-50,100])\n",
    "    plt.ylim([-50,100])\n",
    "    plt.grid()\n",
    "    plt.xlabel('Predicted Output')\n",
    "    plt.ylabel('Actual Output')\n",
    "    plt.title('Input Output Characterstics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_plot = interactive(plot_pred_actual, metabolite=(0, inps_test_.shape[1] - 1), simulation = (0, inps_test_.shape[0] - 1), model = (0,1))\n",
    "output = interactive_plot.children[-1]\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biocomp-NN-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
