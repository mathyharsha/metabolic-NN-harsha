{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"./data/2025-07-01_full_training_data_99993_samples.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"Transformer layer for metabolic modeling\"\"\"\n",
    "    def __init__(self,vocab_size=115,dim=5):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = dim\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.W_k = nn.Linear(dim,dim)\n",
    "        self.W_q = nn.Linear(dim,dim)\n",
    "        self.W_v = nn.Linear(dim,dim)\n",
    "        self.W_c = nn.Linear(vocab_size,vocab_size)\n",
    "\n",
    "    def scaled_dot_product_attention(self,keys,queries,values):\n",
    "        # Find the product if K and Q transpose and divide by the square root of the model dimension (d_model)\n",
    "\n",
    "        pre_softmax_attention_matrix = torch.einsum('bij,bkj->bik', keys,queries)/np.sqrt(self.d_model)\n",
    "        attention_matrix = torch.softmax(pre_softmax_attention_matrix,dim=-1)\n",
    "        attention_output = torch.einsum( 'bij,bjk->bik' , attention_matrix, values)\n",
    "\n",
    "        return attention_output, attention_matrix\n",
    "\n",
    "    def forward(self,x,c):\n",
    "        norm_x = self.layer_norm(x)\n",
    "        modified_c = self.W_c(c)\n",
    "\n",
    "        Q = self.W_k(norm_x)\n",
    "        K = self.W_q(norm_x)\n",
    "        V = self.W_v(norm_x)\n",
    "\n",
    "        attention_output, attention_matrix = self.scaled_dot_product_attention(Q,K,V)\n",
    "\n",
    "        #print(attention_matrix.size(),modified_c.size())\n",
    "\n",
    "        attended_c = torch.einsum('bij,bjk->bik',attention_matrix,modified_c)\n",
    "        \n",
    "        #print(c.size(),attended_c.size())\n",
    "\n",
    "        output_x = attention_output + x \n",
    "        output_c = attended_c + c\n",
    "\n",
    "        return output_x, output_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model,dropout=0.1):\n",
    "        super(FeedForwardBlock, self).__init__()\n",
    "\n",
    "        self.d_model = d_model+1\n",
    "        self.inner_dim = 6*(d_model+1)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(self.d_model)\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(self.d_model,self.inner_dim)\n",
    "        self.linear_layer_2 = nn.Linear(self.inner_dim,self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x,c):\n",
    "\n",
    "        y = torch.cat((x,c),2)\n",
    "        \n",
    "        norm_y = self.layer_norm(y)\n",
    "        \n",
    "        norm_y = self.linear_layer_1(norm_y)\n",
    "        norm_y = F.relu(norm_y)\n",
    "        norm_y = self.linear_layer_2(norm_y)\n",
    "\n",
    "        return norm_y + y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Embedding layer + Attention Block + FeedForward Layer\"\"\"\n",
    "    def __init__(self,vocab_size=115,dim=5):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.d_model = dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.inp_embedding = nn.Embedding(vocab_size,dim)\n",
    "\n",
    "        self.attention_block = AttentionBlock(vocab_size,dim)\n",
    "\n",
    "        self.feedforward_block = FeedForwardBlock(dim)\n",
    "\n",
    "        self.linear_layer_1 = nn.Linear(vocab_size,vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    \n",
    "    def forward(self,c):\n",
    "\n",
    "        batch_size, vocab_size, _ = c.size()\n",
    "\n",
    "        # y = torch.randint(0, vocab_size, (batch_size, vocab_size))\n",
    "        # for k in range(vocab_size):\n",
    "        #     y[:,k] = k\n",
    "\n",
    "        y = torch.arange(vocab_size).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        x = self.inp_embedding(y)\n",
    "        # print(x.size())\n",
    "        \n",
    "        output_x, output_c = self.attention_block(x,c)\n",
    "\n",
    "        output_y = self.feedforward_block(output_x,output_c)\n",
    "        \n",
    "\n",
    "        return output_y[:,:,-1].unsqueeze(-1)\n",
    "\n",
    "        #return output_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (575x1 and 115x115)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[402]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m c = torch.randn(batch_size,seq_len,\u001b[32m1\u001b[39m)\n\u001b[32m      8\u001b[39m transformer = TransformerBlock(seq_len,d_model)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m output = \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m ,\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOutput Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/metabolic-NN-harshat/metabolic-NN-harsha/biocomp-NN/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/metabolic-NN-harshat/metabolic-NN-harsha/biocomp-NN/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[400]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, c)\u001b[39m\n\u001b[32m     38\u001b[39m x = \u001b[38;5;28mself\u001b[39m.inp_embedding(y)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# print(x.size())\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m output_x, output_c = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m output_y = \u001b[38;5;28mself\u001b[39m.feedforward_block(output_x,output_c)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output_y[:,:,-\u001b[32m1\u001b[39m].unsqueeze(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/metabolic-NN-harshat/metabolic-NN-harsha/biocomp-NN/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/metabolic-NN-harshat/metabolic-NN-harsha/biocomp-NN/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[398]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mAttentionBlock.forward\u001b[39m\u001b[34m(self, x, c)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,c):\n\u001b[32m     26\u001b[39m     norm_x = \u001b[38;5;28mself\u001b[39m.layer_norm(x)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     modified_c = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     Q = \u001b[38;5;28mself\u001b[39m.W_k(norm_x)\n\u001b[32m     30\u001b[39m     K = \u001b[38;5;28mself\u001b[39m.W_q(norm_x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/metabolic-NN-harshat/metabolic-NN-harsha/biocomp-NN/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/metabolic-NN-harshat/metabolic-NN-harsha/biocomp-NN/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/metabolic-NN-harshat/metabolic-NN-harsha/biocomp-NN/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (575x1 and 115x115)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a simple attention block\n",
    "d_model = 5\n",
    "seq_len = 115\n",
    "batch_size = 5\n",
    "\n",
    "# A randomly created Dataset\n",
    "c = torch.randn(batch_size,seq_len,1)\n",
    "transformer = TransformerBlock(seq_len,d_model)\n",
    "\n",
    "output = transformer(c)\n",
    "\n",
    "print(f\"Input Shape: {c.shape}\" ,f\"Output Shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 115])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.transpose(-2,-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data_(filename):\n",
    "    \"\"\"Load and preprocess the training data\"\"\"\n",
    "    input_cols = [\n",
    "        'EX_glc__D_e', 'EX_fru_e', 'EX_lac__D_e', 'EX_pyr_e', 'EX_ac_e',\n",
    "        'EX_akg_e', 'EX_succ_e', 'EX_fum_e', 'EX_mal__L_e', 'EX_etoh_e',\n",
    "        'EX_acald_e', 'EX_for_e', 'EX_gln__L_e', 'EX_glu__L_e',\n",
    "        'EX_co2_e', 'EX_h_e', 'EX_h2o_e', 'EX_nh4_e', 'EX_o2_e', 'EX_pi_e'\n",
    "    ]\n",
    "\n",
    "    output_cols = [\n",
    "        'ACALD_flux',\n",
    "        'ACALDt_flux',\n",
    "        'ACKr_flux',\n",
    "        'ACONTa_flux',\n",
    "        'ACONTb_flux',\n",
    "        'ACt2r_flux',\n",
    "        'ADK1_flux',\n",
    "        'AKGDH_flux',\n",
    "        'AKGt2r_flux',\n",
    "        'ALCD2x_flux',\n",
    "        'ATPM_flux',\n",
    "        'ATPS4r_flux',\n",
    "        'Biomass_Ecoli_core_flux',\n",
    "        'CO2t_flux',\n",
    "        'CS_flux',\n",
    "        'CYTBD_flux',\n",
    "        'D_LACt2_flux',\n",
    "        'ENO_flux',\n",
    "        'ETOHt2r_flux',\n",
    "        'EX_ac_e_flux',\n",
    "        'EX_acald_e_flux',\n",
    "        'EX_akg_e_flux',\n",
    "        'EX_co2_e_flux',\n",
    "        'EX_etoh_e_flux',\n",
    "        'EX_for_e_flux',\n",
    "        'EX_fru_e_flux',\n",
    "        'EX_fum_e_flux',\n",
    "        'EX_glc__D_e_flux',\n",
    "        'EX_gln__L_e_flux',\n",
    "        'EX_glu__L_e_flux',\n",
    "        'EX_h_e_flux',\n",
    "        'EX_h2o_e_flux',\n",
    "        'EX_lac__D_e_flux',\n",
    "        'EX_mal__L_e_flux',\n",
    "        'EX_nh4_e_flux',\n",
    "        'EX_o2_e_flux',\n",
    "        'EX_pi_e_flux',\n",
    "        'EX_pyr_e_flux',\n",
    "        'EX_succ_e_flux',\n",
    "        'FBA_flux',\n",
    "        'FBP_flux',\n",
    "        'FORt2_flux',\n",
    "        'FORti_flux',\n",
    "        'FRD7_flux',\n",
    "        'FRUpts2_flux',\n",
    "        'FUM_flux',\n",
    "        'FUMt2_2_flux',\n",
    "        'G6PDH2r_flux',\n",
    "        'GAPD_flux',\n",
    "        'GLCpts_flux',\n",
    "        'GLNS_flux',\n",
    "        'GLNabc_flux',\n",
    "        'GLUDy_flux',\n",
    "        'GLUN_flux',\n",
    "        'GLUSy_flux',\n",
    "        'GLUt2r_flux',\n",
    "        'GND_flux',\n",
    "        'H2Ot_flux',\n",
    "        'ICDHyr_flux',\n",
    "        'ICL_flux',\n",
    "        'LDH_D_flux',\n",
    "        'MALS_flux',\n",
    "        'MALt2_2_flux',\n",
    "        'MDH_flux',\n",
    "        'ME1_flux',\n",
    "        'ME2_flux',\n",
    "        'NADH16_flux',\n",
    "        'NADTRHD_flux',\n",
    "        'NH4t_flux',\n",
    "        'O2t_flux',\n",
    "        'PDH_flux',\n",
    "        'PFK_flux',\n",
    "        'PFL_flux',\n",
    "        'PGI_flux',\n",
    "        'PGK_flux',\n",
    "        'PGL_flux',\n",
    "        'PGM_flux',\n",
    "        'PIt2r_flux',\n",
    "        'PPC_flux',\n",
    "        'PPCK_flux',\n",
    "        'PPS_flux',\n",
    "        'PTAr_flux',\n",
    "        'PYK_flux',\n",
    "        'PYRt2_flux',\n",
    "        'RPE_flux',\n",
    "        'RPI_flux',\n",
    "        'SUCCt2_2_flux',\n",
    "        'SUCCt3_flux',\n",
    "        'SUCDi_flux',\n",
    "        'SUCOAS_flux',\n",
    "        'TALA_flux',\n",
    "        'THD2_flux',\n",
    "        'TKT1_flux',\n",
    "        'TKT2_flux',\n",
    "        'TPI_flux'\n",
    "    ]\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Fill missing inputs with 0 (i.e., not uptaken)\n",
    "    df[input_cols] = df[input_cols].fillna(0)\n",
    "\n",
    "    print(f\"\\nLoaded data with {len(df)} samples from {filename}\")\n",
    "    print(f\"Total outputs: {len(output_cols)}\")\n",
    "\n",
    "    X = df[input_cols].values.astype(np.float32)\n",
    "    y = df[output_cols].values.astype(np.float32)\n",
    "\n",
    "    total_len = X.shape[1] + y.shape[1]\n",
    "\n",
    "    inps = np.zeros((X.shape[0],total_len))\n",
    "    outs = np.zeros((y.shape[0],total_len))\n",
    "\n",
    "    inps[:,:X.shape[1]] = X\n",
    "    outs[:,X.shape[1]:] = y\n",
    "\n",
    "    return inps, outs, input_cols + output_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded data with 99993 samples from ./data/2025-07-01_full_training_data_99993_samples.csv\n",
      "Total outputs: 95\n"
     ]
    }
   ],
   "source": [
    "inps, outs, all_cols = load_and_preprocess_data_(datafile)\n",
    "inps = torch.tensor(inps,dtype=torch.float32)\n",
    "outs = torch.tensor(outs,dtype=torch.float32)\n",
    "total_size = inps.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([99993, 115]), torch.Size([99993, 115]), 115)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps.size(), outs.size(), len(all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some test lines\n",
    "# inp_len = 3\n",
    "# idxs = torch.randint(0,total_size,(batch_size,))\n",
    "\n",
    "# inps.size()\n",
    "# new_inps = inps[idxs,:]\n",
    "# new_outs = outs[idxs,:]\n",
    "# new_inps.size()\n",
    "# new_inps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    average_losses = []\n",
    "    vocab_size = 115\n",
    "    d_model = 15\n",
    "    batch_size = 10\n",
    "    learning_rate = 1e-3\n",
    "    num_epochs = 900\n",
    "    num_batches = 30\n",
    "\n",
    "    model = TransformerBlock(vocab_size,d_model)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "\n",
    "            idxs = torch.randint(0,total_size,(batch_size,))\n",
    "\n",
    "            batch_inps = inps[idxs,:].unsqueeze(-1)\n",
    "            batch_targets = outs[idxs,:].unsqueeze(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print(batch_inps.size(),batch_targets.size())\n",
    "            # print(batch_inps.dtype, batch_targets.dtype)\n",
    "            batch_outs = model(batch_inps)\n",
    "\n",
    "            # print(batch_outs.size(),batch_targets.size())\n",
    "\n",
    "            loss = criterion(batch_outs,batch_targets)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 20 == 0:\n",
    "                avg_loss = total_loss / (batch_idx + 1)\n",
    "\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{num_batches}]\",\n",
    "                      f\"Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        avg_epoch_loss = total_loss / num_batches\n",
    "        average_losses.append(avg_epoch_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Loss: {avg_epoch_loss: .4f}\")\n",
    "    \n",
    "    print('Training Completed')\n",
    "    return average_losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_per_epoch, title=\"Training Loss\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plot loss per epoch\n",
    "    \n",
    "    Args:\n",
    "        loss_per_epoch (list): List of loss values for each epoch\n",
    "        title (str): Plot title\n",
    "        save_path (str, optional): Path to save the plot\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(loss_per_epoch) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, loss_per_epoch, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "    plt.title(title, fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add some styling\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "#     trained_model = train_model()\n",
    "#     plot_loss(average_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/900], Batch [20/30] Avg Loss: 1043.0478\n",
      "Epoch [1/900] completed. Average Loss:  999.7029\n",
      "Epoch [2/900], Batch [20/30] Avg Loss: 758.8739\n",
      "Epoch [2/900] completed. Average Loss:  726.1861\n",
      "Epoch [3/900], Batch [20/30] Avg Loss: 630.4882\n",
      "Epoch [3/900] completed. Average Loss:  619.3020\n",
      "Epoch [4/900], Batch [20/30] Avg Loss: 572.5059\n",
      "Epoch [4/900] completed. Average Loss:  562.0385\n",
      "Epoch [5/900], Batch [20/30] Avg Loss: 502.3204\n",
      "Epoch [5/900] completed. Average Loss:  485.8040\n",
      "Epoch [6/900], Batch [20/30] Avg Loss: 418.0633\n",
      "Epoch [6/900] completed. Average Loss:  405.4748\n",
      "Epoch [7/900], Batch [20/30] Avg Loss: 362.7191\n",
      "Epoch [7/900] completed. Average Loss:  361.8923\n",
      "Epoch [8/900], Batch [20/30] Avg Loss: 341.0194\n",
      "Epoch [8/900] completed. Average Loss:  346.1753\n",
      "Epoch [9/900], Batch [20/30] Avg Loss: 339.7503\n",
      "Epoch [9/900] completed. Average Loss:  344.0731\n",
      "Epoch [10/900], Batch [20/30] Avg Loss: 347.5243\n",
      "Epoch [10/900] completed. Average Loss:  343.6894\n",
      "Epoch [11/900], Batch [20/30] Avg Loss: 335.5801\n",
      "Epoch [11/900] completed. Average Loss:  336.2949\n",
      "Epoch [12/900], Batch [20/30] Avg Loss: 338.8762\n",
      "Epoch [12/900] completed. Average Loss:  339.8389\n",
      "Epoch [13/900], Batch [20/30] Avg Loss: 335.1866\n",
      "Epoch [13/900] completed. Average Loss:  337.9226\n",
      "Epoch [14/900], Batch [20/30] Avg Loss: 354.1306\n",
      "Epoch [14/900] completed. Average Loss:  352.7499\n",
      "Epoch [15/900], Batch [20/30] Avg Loss: 335.2880\n",
      "Epoch [15/900] completed. Average Loss:  331.1123\n",
      "Epoch [16/900], Batch [20/30] Avg Loss: 319.0128\n",
      "Epoch [16/900] completed. Average Loss:  320.1986\n",
      "Epoch [17/900], Batch [20/30] Avg Loss: 297.2987\n",
      "Epoch [17/900] completed. Average Loss:  295.5031\n",
      "Epoch [18/900], Batch [20/30] Avg Loss: 285.1535\n",
      "Epoch [18/900] completed. Average Loss:  293.1452\n",
      "Epoch [19/900], Batch [20/30] Avg Loss: 276.8686\n",
      "Epoch [19/900] completed. Average Loss:  275.4826\n",
      "Epoch [20/900], Batch [20/30] Avg Loss: 272.7700\n",
      "Epoch [20/900] completed. Average Loss:  264.9945\n",
      "Epoch [21/900], Batch [20/30] Avg Loss: 262.7342\n",
      "Epoch [21/900] completed. Average Loss:  262.5797\n",
      "Epoch [22/900], Batch [20/30] Avg Loss: 251.4533\n",
      "Epoch [22/900] completed. Average Loss:  252.0875\n",
      "Epoch [23/900], Batch [20/30] Avg Loss: 248.9626\n",
      "Epoch [23/900] completed. Average Loss:  238.4332\n",
      "Epoch [24/900], Batch [20/30] Avg Loss: 225.6776\n",
      "Epoch [24/900] completed. Average Loss:  232.7823\n",
      "Epoch [25/900], Batch [20/30] Avg Loss: 219.9238\n",
      "Epoch [25/900] completed. Average Loss:  217.5825\n",
      "Epoch [26/900], Batch [20/30] Avg Loss: 225.9876\n",
      "Epoch [26/900] completed. Average Loss:  223.4486\n",
      "Epoch [27/900], Batch [20/30] Avg Loss: 215.3967\n",
      "Epoch [27/900] completed. Average Loss:  216.2032\n",
      "Epoch [28/900], Batch [20/30] Avg Loss: 211.3034\n",
      "Epoch [28/900] completed. Average Loss:  211.2794\n",
      "Epoch [29/900], Batch [20/30] Avg Loss: 216.2762\n",
      "Epoch [29/900] completed. Average Loss:  215.2605\n",
      "Epoch [30/900], Batch [20/30] Avg Loss: 199.2827\n",
      "Epoch [30/900] completed. Average Loss:  201.2111\n",
      "Epoch [31/900], Batch [20/30] Avg Loss: 205.5808\n",
      "Epoch [31/900] completed. Average Loss:  207.7999\n",
      "Epoch [32/900], Batch [20/30] Avg Loss: 178.9971\n",
      "Epoch [32/900] completed. Average Loss:  187.9742\n",
      "Epoch [33/900], Batch [20/30] Avg Loss: 183.4238\n",
      "Epoch [33/900] completed. Average Loss:  185.3396\n",
      "Epoch [34/900], Batch [20/30] Avg Loss: 180.2116\n",
      "Epoch [34/900] completed. Average Loss:  179.8515\n",
      "Epoch [35/900], Batch [20/30] Avg Loss: 179.3874\n",
      "Epoch [35/900] completed. Average Loss:  179.7823\n",
      "Epoch [36/900], Batch [20/30] Avg Loss: 163.5768\n",
      "Epoch [36/900] completed. Average Loss:  162.9712\n",
      "Epoch [37/900], Batch [20/30] Avg Loss: 164.1608\n",
      "Epoch [37/900] completed. Average Loss:  164.1425\n",
      "Epoch [38/900], Batch [20/30] Avg Loss: 149.0091\n",
      "Epoch [38/900] completed. Average Loss:  148.9347\n",
      "Epoch [39/900], Batch [20/30] Avg Loss: 158.4444\n",
      "Epoch [39/900] completed. Average Loss:  152.4640\n",
      "Epoch [40/900], Batch [20/30] Avg Loss: 127.6990\n",
      "Epoch [40/900] completed. Average Loss:  123.6466\n",
      "Epoch [41/900], Batch [20/30] Avg Loss: 125.5168\n",
      "Epoch [41/900] completed. Average Loss:  123.2623\n",
      "Epoch [42/900], Batch [20/30] Avg Loss: 113.5805\n",
      "Epoch [42/900] completed. Average Loss:  110.6836\n",
      "Epoch [43/900], Batch [20/30] Avg Loss: 103.2060\n",
      "Epoch [43/900] completed. Average Loss:  103.0216\n",
      "Epoch [44/900], Batch [20/30] Avg Loss: 93.6672\n",
      "Epoch [44/900] completed. Average Loss:  91.3707\n",
      "Epoch [45/900], Batch [20/30] Avg Loss: 89.0183\n",
      "Epoch [45/900] completed. Average Loss:  87.6766\n",
      "Epoch [46/900], Batch [20/30] Avg Loss: 76.8135\n",
      "Epoch [46/900] completed. Average Loss:  75.4938\n",
      "Epoch [47/900], Batch [20/30] Avg Loss: 73.7834\n",
      "Epoch [47/900] completed. Average Loss:  75.5025\n",
      "Epoch [48/900], Batch [20/30] Avg Loss: 68.6390\n",
      "Epoch [48/900] completed. Average Loss:  69.3532\n",
      "Epoch [49/900], Batch [20/30] Avg Loss: 68.8936\n",
      "Epoch [49/900] completed. Average Loss:  69.4476\n",
      "Epoch [50/900], Batch [20/30] Avg Loss: 63.7469\n",
      "Epoch [50/900] completed. Average Loss:  59.2514\n",
      "Epoch [51/900], Batch [20/30] Avg Loss: 57.8986\n",
      "Epoch [51/900] completed. Average Loss:  57.5234\n",
      "Epoch [52/900], Batch [20/30] Avg Loss: 51.0622\n",
      "Epoch [52/900] completed. Average Loss:  49.0914\n",
      "Epoch [53/900], Batch [20/30] Avg Loss: 55.9418\n",
      "Epoch [53/900] completed. Average Loss:  51.5506\n",
      "Epoch [54/900], Batch [20/30] Avg Loss: 46.9079\n",
      "Epoch [54/900] completed. Average Loss:  49.3742\n",
      "Epoch [55/900], Batch [20/30] Avg Loss: 47.8775\n",
      "Epoch [55/900] completed. Average Loss:  45.0179\n",
      "Epoch [56/900], Batch [20/30] Avg Loss: 49.7296\n",
      "Epoch [56/900] completed. Average Loss:  48.1789\n",
      "Epoch [57/900], Batch [20/30] Avg Loss: 51.6138\n",
      "Epoch [57/900] completed. Average Loss:  49.4232\n",
      "Epoch [58/900], Batch [20/30] Avg Loss: 42.7254\n",
      "Epoch [58/900] completed. Average Loss:  42.2001\n",
      "Epoch [59/900], Batch [20/30] Avg Loss: 48.4968\n",
      "Epoch [59/900] completed. Average Loss:  48.5495\n",
      "Epoch [60/900], Batch [20/30] Avg Loss: 43.2838\n",
      "Epoch [60/900] completed. Average Loss:  44.2662\n",
      "Epoch [61/900], Batch [20/30] Avg Loss: 47.6526\n",
      "Epoch [61/900] completed. Average Loss:  46.7604\n",
      "Epoch [62/900], Batch [20/30] Avg Loss: 44.2594\n",
      "Epoch [62/900] completed. Average Loss:  43.9524\n",
      "Epoch [63/900], Batch [20/30] Avg Loss: 38.1593\n",
      "Epoch [63/900] completed. Average Loss:  41.3303\n",
      "Epoch [64/900], Batch [20/30] Avg Loss: 39.5856\n",
      "Epoch [64/900] completed. Average Loss:  41.4036\n",
      "Epoch [65/900], Batch [20/30] Avg Loss: 43.7777\n",
      "Epoch [65/900] completed. Average Loss:  44.3063\n",
      "Epoch [66/900], Batch [20/30] Avg Loss: 38.3686\n",
      "Epoch [66/900] completed. Average Loss:  38.9730\n",
      "Epoch [67/900], Batch [20/30] Avg Loss: 41.5082\n",
      "Epoch [67/900] completed. Average Loss:  39.5704\n",
      "Epoch [68/900], Batch [20/30] Avg Loss: 40.8591\n",
      "Epoch [68/900] completed. Average Loss:  38.1486\n",
      "Epoch [69/900], Batch [20/30] Avg Loss: 39.7057\n",
      "Epoch [69/900] completed. Average Loss:  41.3118\n",
      "Epoch [70/900], Batch [20/30] Avg Loss: 43.2433\n",
      "Epoch [70/900] completed. Average Loss:  44.4532\n",
      "Epoch [71/900], Batch [20/30] Avg Loss: 38.0548\n",
      "Epoch [71/900] completed. Average Loss:  37.8689\n",
      "Epoch [72/900], Batch [20/30] Avg Loss: 43.2470\n",
      "Epoch [72/900] completed. Average Loss:  46.0143\n",
      "Epoch [73/900], Batch [20/30] Avg Loss: 40.0319\n",
      "Epoch [73/900] completed. Average Loss:  39.4698\n",
      "Epoch [74/900], Batch [20/30] Avg Loss: 41.2070\n",
      "Epoch [74/900] completed. Average Loss:  41.7026\n",
      "Epoch [75/900], Batch [20/30] Avg Loss: 42.7450\n",
      "Epoch [75/900] completed. Average Loss:  41.9337\n",
      "Epoch [76/900], Batch [20/30] Avg Loss: 41.5706\n",
      "Epoch [76/900] completed. Average Loss:  39.6587\n",
      "Epoch [77/900], Batch [20/30] Avg Loss: 35.3586\n",
      "Epoch [77/900] completed. Average Loss:  34.7583\n",
      "Epoch [78/900], Batch [20/30] Avg Loss: 43.4523\n",
      "Epoch [78/900] completed. Average Loss:  45.1640\n",
      "Epoch [79/900], Batch [20/30] Avg Loss: 47.1537\n",
      "Epoch [79/900] completed. Average Loss:  47.2240\n",
      "Epoch [80/900], Batch [20/30] Avg Loss: 42.5728\n",
      "Epoch [80/900] completed. Average Loss:  45.3632\n",
      "Epoch [81/900], Batch [20/30] Avg Loss: 43.9430\n",
      "Epoch [81/900] completed. Average Loss:  43.9392\n",
      "Epoch [82/900], Batch [20/30] Avg Loss: 43.2078\n",
      "Epoch [82/900] completed. Average Loss:  44.4244\n",
      "Epoch [83/900], Batch [20/30] Avg Loss: 43.1662\n",
      "Epoch [83/900] completed. Average Loss:  41.3058\n",
      "Epoch [84/900], Batch [20/30] Avg Loss: 43.1211\n",
      "Epoch [84/900] completed. Average Loss:  42.4448\n",
      "Epoch [85/900], Batch [20/30] Avg Loss: 38.6043\n",
      "Epoch [85/900] completed. Average Loss:  40.1293\n",
      "Epoch [86/900], Batch [20/30] Avg Loss: 39.5091\n",
      "Epoch [86/900] completed. Average Loss:  41.0880\n",
      "Epoch [87/900], Batch [20/30] Avg Loss: 39.3240\n",
      "Epoch [87/900] completed. Average Loss:  41.4641\n",
      "Epoch [88/900], Batch [20/30] Avg Loss: 39.6086\n",
      "Epoch [88/900] completed. Average Loss:  40.0815\n",
      "Epoch [89/900], Batch [20/30] Avg Loss: 41.4991\n",
      "Epoch [89/900] completed. Average Loss:  44.4902\n",
      "Epoch [90/900], Batch [20/30] Avg Loss: 42.8977\n",
      "Epoch [90/900] completed. Average Loss:  41.1240\n",
      "Epoch [91/900], Batch [20/30] Avg Loss: 35.9240\n",
      "Epoch [91/900] completed. Average Loss:  37.9393\n",
      "Epoch [92/900], Batch [20/30] Avg Loss: 42.5879\n",
      "Epoch [92/900] completed. Average Loss:  45.5303\n",
      "Epoch [93/900], Batch [20/30] Avg Loss: 41.6779\n",
      "Epoch [93/900] completed. Average Loss:  41.4727\n",
      "Epoch [94/900], Batch [20/30] Avg Loss: 39.8043\n",
      "Epoch [94/900] completed. Average Loss:  39.6820\n",
      "Epoch [95/900], Batch [20/30] Avg Loss: 38.2342\n",
      "Epoch [95/900] completed. Average Loss:  37.9662\n",
      "Epoch [96/900], Batch [20/30] Avg Loss: 44.9903\n",
      "Epoch [96/900] completed. Average Loss:  42.2135\n",
      "Epoch [97/900], Batch [20/30] Avg Loss: 38.0773\n",
      "Epoch [97/900] completed. Average Loss:  38.9119\n",
      "Epoch [98/900], Batch [20/30] Avg Loss: 43.2023\n",
      "Epoch [98/900] completed. Average Loss:  40.5727\n",
      "Epoch [99/900], Batch [20/30] Avg Loss: 49.0518\n",
      "Epoch [99/900] completed. Average Loss:  47.8509\n",
      "Epoch [100/900], Batch [20/30] Avg Loss: 39.4402\n",
      "Epoch [100/900] completed. Average Loss:  38.8500\n",
      "Epoch [101/900], Batch [20/30] Avg Loss: 39.3020\n",
      "Epoch [101/900] completed. Average Loss:  39.2146\n",
      "Epoch [102/900], Batch [20/30] Avg Loss: 41.1855\n",
      "Epoch [102/900] completed. Average Loss:  42.9682\n",
      "Epoch [103/900], Batch [20/30] Avg Loss: 44.2880\n",
      "Epoch [103/900] completed. Average Loss:  41.5743\n",
      "Epoch [104/900], Batch [20/30] Avg Loss: 41.0833\n",
      "Epoch [104/900] completed. Average Loss:  43.1080\n",
      "Epoch [105/900], Batch [20/30] Avg Loss: 40.5008\n",
      "Epoch [105/900] completed. Average Loss:  41.0997\n",
      "Epoch [106/900], Batch [20/30] Avg Loss: 38.6993\n",
      "Epoch [106/900] completed. Average Loss:  40.4129\n",
      "Epoch [107/900], Batch [20/30] Avg Loss: 40.8826\n",
      "Epoch [107/900] completed. Average Loss:  40.2102\n",
      "Epoch [108/900], Batch [20/30] Avg Loss: 41.1312\n",
      "Epoch [108/900] completed. Average Loss:  40.1441\n",
      "Epoch [109/900], Batch [20/30] Avg Loss: 40.7692\n",
      "Epoch [109/900] completed. Average Loss:  39.9684\n",
      "Epoch [110/900], Batch [20/30] Avg Loss: 39.0586\n",
      "Epoch [110/900] completed. Average Loss:  41.5116\n",
      "Epoch [111/900], Batch [20/30] Avg Loss: 42.1555\n",
      "Epoch [111/900] completed. Average Loss:  41.5064\n",
      "Epoch [112/900], Batch [20/30] Avg Loss: 33.7643\n",
      "Epoch [112/900] completed. Average Loss:  36.3638\n",
      "Epoch [113/900], Batch [20/30] Avg Loss: 37.6988\n",
      "Epoch [113/900] completed. Average Loss:  37.7305\n",
      "Epoch [114/900], Batch [20/30] Avg Loss: 41.1661\n",
      "Epoch [114/900] completed. Average Loss:  41.9081\n",
      "Epoch [115/900], Batch [20/30] Avg Loss: 42.9670\n",
      "Epoch [115/900] completed. Average Loss:  39.9524\n",
      "Epoch [116/900], Batch [20/30] Avg Loss: 43.1327\n",
      "Epoch [116/900] completed. Average Loss:  44.2759\n",
      "Epoch [117/900], Batch [20/30] Avg Loss: 41.1015\n",
      "Epoch [117/900] completed. Average Loss:  42.7142\n",
      "Epoch [118/900], Batch [20/30] Avg Loss: 36.2162\n",
      "Epoch [118/900] completed. Average Loss:  38.5641\n",
      "Epoch [119/900], Batch [20/30] Avg Loss: 40.6091\n",
      "Epoch [119/900] completed. Average Loss:  40.8434\n",
      "Epoch [120/900], Batch [20/30] Avg Loss: 45.1370\n",
      "Epoch [120/900] completed. Average Loss:  42.9543\n",
      "Epoch [121/900], Batch [20/30] Avg Loss: 41.4521\n",
      "Epoch [121/900] completed. Average Loss:  42.2836\n",
      "Epoch [122/900], Batch [20/30] Avg Loss: 42.4980\n",
      "Epoch [122/900] completed. Average Loss:  42.2233\n",
      "Epoch [123/900], Batch [20/30] Avg Loss: 40.6769\n",
      "Epoch [123/900] completed. Average Loss:  41.0055\n",
      "Epoch [124/900], Batch [20/30] Avg Loss: 40.0779\n",
      "Epoch [124/900] completed. Average Loss:  41.5930\n",
      "Epoch [125/900], Batch [20/30] Avg Loss: 36.6811\n",
      "Epoch [125/900] completed. Average Loss:  38.1304\n",
      "Epoch [126/900], Batch [20/30] Avg Loss: 38.1207\n",
      "Epoch [126/900] completed. Average Loss:  40.7716\n",
      "Epoch [127/900], Batch [20/30] Avg Loss: 42.7329\n",
      "Epoch [127/900] completed. Average Loss:  41.9049\n",
      "Epoch [128/900], Batch [20/30] Avg Loss: 39.7235\n",
      "Epoch [128/900] completed. Average Loss:  39.0096\n",
      "Epoch [129/900], Batch [20/30] Avg Loss: 40.1116\n",
      "Epoch [129/900] completed. Average Loss:  40.4234\n",
      "Epoch [130/900], Batch [20/30] Avg Loss: 39.4970\n",
      "Epoch [130/900] completed. Average Loss:  40.0978\n",
      "Epoch [131/900], Batch [20/30] Avg Loss: 45.6311\n",
      "Epoch [131/900] completed. Average Loss:  42.1561\n",
      "Epoch [132/900], Batch [20/30] Avg Loss: 43.0372\n",
      "Epoch [132/900] completed. Average Loss:  40.4019\n",
      "Epoch [133/900], Batch [20/30] Avg Loss: 43.9952\n",
      "Epoch [133/900] completed. Average Loss:  45.5335\n",
      "Epoch [134/900], Batch [20/30] Avg Loss: 39.6124\n",
      "Epoch [134/900] completed. Average Loss:  41.0580\n",
      "Epoch [135/900], Batch [20/30] Avg Loss: 46.3124\n",
      "Epoch [135/900] completed. Average Loss:  45.8412\n",
      "Epoch [136/900], Batch [20/30] Avg Loss: 43.8975\n",
      "Epoch [136/900] completed. Average Loss:  41.0859\n",
      "Epoch [137/900], Batch [20/30] Avg Loss: 42.5182\n",
      "Epoch [137/900] completed. Average Loss:  42.6302\n",
      "Epoch [138/900], Batch [20/30] Avg Loss: 40.7382\n",
      "Epoch [138/900] completed. Average Loss:  45.4841\n",
      "Epoch [139/900], Batch [20/30] Avg Loss: 39.2218\n",
      "Epoch [139/900] completed. Average Loss:  40.7116\n",
      "Epoch [140/900], Batch [20/30] Avg Loss: 36.7670\n",
      "Epoch [140/900] completed. Average Loss:  39.4658\n",
      "Epoch [141/900], Batch [20/30] Avg Loss: 37.0106\n",
      "Epoch [141/900] completed. Average Loss:  34.6835\n",
      "Epoch [142/900], Batch [20/30] Avg Loss: 40.0127\n",
      "Epoch [142/900] completed. Average Loss:  42.5745\n",
      "Epoch [143/900], Batch [20/30] Avg Loss: 47.4919\n",
      "Epoch [143/900] completed. Average Loss:  45.4897\n",
      "Epoch [144/900], Batch [20/30] Avg Loss: 40.8825\n",
      "Epoch [144/900] completed. Average Loss:  40.4038\n",
      "Epoch [145/900], Batch [20/30] Avg Loss: 39.7078\n",
      "Epoch [145/900] completed. Average Loss:  40.2962\n",
      "Epoch [146/900], Batch [20/30] Avg Loss: 35.5996\n",
      "Epoch [146/900] completed. Average Loss:  38.2348\n",
      "Epoch [147/900], Batch [20/30] Avg Loss: 43.7303\n",
      "Epoch [147/900] completed. Average Loss:  41.9296\n",
      "Epoch [148/900], Batch [20/30] Avg Loss: 52.5859\n",
      "Epoch [148/900] completed. Average Loss:  47.8426\n",
      "Epoch [149/900], Batch [20/30] Avg Loss: 43.0453\n",
      "Epoch [149/900] completed. Average Loss:  40.9403\n",
      "Epoch [150/900], Batch [20/30] Avg Loss: 37.9265\n",
      "Epoch [150/900] completed. Average Loss:  39.0653\n",
      "Epoch [151/900], Batch [20/30] Avg Loss: 41.1271\n",
      "Epoch [151/900] completed. Average Loss:  38.3558\n",
      "Epoch [152/900], Batch [20/30] Avg Loss: 36.0134\n",
      "Epoch [152/900] completed. Average Loss:  40.5060\n",
      "Epoch [153/900], Batch [20/30] Avg Loss: 36.0684\n",
      "Epoch [153/900] completed. Average Loss:  38.0155\n",
      "Epoch [154/900], Batch [20/30] Avg Loss: 42.4822\n",
      "Epoch [154/900] completed. Average Loss:  40.1224\n",
      "Epoch [155/900], Batch [20/30] Avg Loss: 44.3324\n",
      "Epoch [155/900] completed. Average Loss:  44.1877\n",
      "Epoch [156/900], Batch [20/30] Avg Loss: 48.1892\n",
      "Epoch [156/900] completed. Average Loss:  42.2908\n",
      "Epoch [157/900], Batch [20/30] Avg Loss: 38.1747\n",
      "Epoch [157/900] completed. Average Loss:  38.2272\n",
      "Epoch [158/900], Batch [20/30] Avg Loss: 39.2906\n",
      "Epoch [158/900] completed. Average Loss:  39.5577\n",
      "Epoch [159/900], Batch [20/30] Avg Loss: 42.1932\n",
      "Epoch [159/900] completed. Average Loss:  44.3879\n",
      "Epoch [160/900], Batch [20/30] Avg Loss: 43.8462\n",
      "Epoch [160/900] completed. Average Loss:  42.1534\n",
      "Epoch [161/900], Batch [20/30] Avg Loss: 41.5070\n",
      "Epoch [161/900] completed. Average Loss:  41.1921\n",
      "Epoch [162/900], Batch [20/30] Avg Loss: 36.8581\n",
      "Epoch [162/900] completed. Average Loss:  37.2728\n",
      "Epoch [163/900], Batch [20/30] Avg Loss: 42.2525\n",
      "Epoch [163/900] completed. Average Loss:  43.0514\n",
      "Epoch [164/900], Batch [20/30] Avg Loss: 40.6718\n",
      "Epoch [164/900] completed. Average Loss:  39.6468\n",
      "Epoch [165/900], Batch [20/30] Avg Loss: 36.3839\n",
      "Epoch [165/900] completed. Average Loss:  35.7222\n",
      "Epoch [166/900], Batch [20/30] Avg Loss: 41.5865\n",
      "Epoch [166/900] completed. Average Loss:  42.6685\n",
      "Epoch [167/900], Batch [20/30] Avg Loss: 44.3054\n",
      "Epoch [167/900] completed. Average Loss:  43.1034\n",
      "Epoch [168/900], Batch [20/30] Avg Loss: 39.5148\n",
      "Epoch [168/900] completed. Average Loss:  41.5549\n",
      "Epoch [169/900], Batch [20/30] Avg Loss: 45.9164\n",
      "Epoch [169/900] completed. Average Loss:  44.2178\n",
      "Epoch [170/900], Batch [20/30] Avg Loss: 38.2243\n",
      "Epoch [170/900] completed. Average Loss:  38.7397\n",
      "Epoch [171/900], Batch [20/30] Avg Loss: 46.7567\n",
      "Epoch [171/900] completed. Average Loss:  46.7432\n",
      "Epoch [172/900], Batch [20/30] Avg Loss: 34.5669\n",
      "Epoch [172/900] completed. Average Loss:  39.7190\n",
      "Epoch [173/900], Batch [20/30] Avg Loss: 39.3972\n",
      "Epoch [173/900] completed. Average Loss:  41.6344\n",
      "Epoch [174/900], Batch [20/30] Avg Loss: 37.2960\n",
      "Epoch [174/900] completed. Average Loss:  38.1840\n",
      "Epoch [175/900], Batch [20/30] Avg Loss: 41.4087\n",
      "Epoch [175/900] completed. Average Loss:  40.0630\n",
      "Epoch [176/900], Batch [20/30] Avg Loss: 39.5413\n",
      "Epoch [176/900] completed. Average Loss:  40.3642\n",
      "Epoch [177/900], Batch [20/30] Avg Loss: 39.4127\n",
      "Epoch [177/900] completed. Average Loss:  40.0054\n",
      "Epoch [178/900], Batch [20/30] Avg Loss: 38.6038\n",
      "Epoch [178/900] completed. Average Loss:  39.0189\n",
      "Epoch [179/900], Batch [20/30] Avg Loss: 37.5439\n",
      "Epoch [179/900] completed. Average Loss:  37.9085\n",
      "Epoch [180/900], Batch [20/30] Avg Loss: 43.7295\n",
      "Epoch [180/900] completed. Average Loss:  41.7062\n",
      "Epoch [181/900], Batch [20/30] Avg Loss: 42.3038\n",
      "Epoch [181/900] completed. Average Loss:  44.7221\n",
      "Epoch [182/900], Batch [20/30] Avg Loss: 40.5230\n",
      "Epoch [182/900] completed. Average Loss:  40.2910\n",
      "Epoch [183/900], Batch [20/30] Avg Loss: 41.4773\n",
      "Epoch [183/900] completed. Average Loss:  39.4301\n",
      "Epoch [184/900], Batch [20/30] Avg Loss: 42.8253\n",
      "Epoch [184/900] completed. Average Loss:  40.7096\n",
      "Epoch [185/900], Batch [20/30] Avg Loss: 45.3898\n",
      "Epoch [185/900] completed. Average Loss:  42.2442\n",
      "Epoch [186/900], Batch [20/30] Avg Loss: 41.8564\n",
      "Epoch [186/900] completed. Average Loss:  40.2527\n",
      "Epoch [187/900], Batch [20/30] Avg Loss: 40.9899\n",
      "Epoch [187/900] completed. Average Loss:  41.2289\n",
      "Epoch [188/900], Batch [20/30] Avg Loss: 41.2335\n",
      "Epoch [188/900] completed. Average Loss:  44.2057\n",
      "Epoch [189/900], Batch [20/30] Avg Loss: 42.1597\n",
      "Epoch [189/900] completed. Average Loss:  40.8870\n",
      "Epoch [190/900], Batch [20/30] Avg Loss: 45.0075\n",
      "Epoch [190/900] completed. Average Loss:  42.3188\n",
      "Epoch [191/900], Batch [20/30] Avg Loss: 37.0280\n",
      "Epoch [191/900] completed. Average Loss:  39.3974\n",
      "Epoch [192/900], Batch [20/30] Avg Loss: 43.1254\n",
      "Epoch [192/900] completed. Average Loss:  43.9753\n",
      "Epoch [193/900], Batch [20/30] Avg Loss: 38.6474\n",
      "Epoch [193/900] completed. Average Loss:  38.0849\n",
      "Epoch [194/900], Batch [20/30] Avg Loss: 43.5379\n",
      "Epoch [194/900] completed. Average Loss:  43.4556\n",
      "Epoch [195/900], Batch [20/30] Avg Loss: 40.7938\n",
      "Epoch [195/900] completed. Average Loss:  40.6167\n",
      "Epoch [196/900], Batch [20/30] Avg Loss: 38.4707\n",
      "Epoch [196/900] completed. Average Loss:  38.1005\n",
      "Epoch [197/900], Batch [20/30] Avg Loss: 40.5723\n",
      "Epoch [197/900] completed. Average Loss:  42.1557\n",
      "Epoch [198/900], Batch [20/30] Avg Loss: 46.8661\n",
      "Epoch [198/900] completed. Average Loss:  48.2149\n",
      "Epoch [199/900], Batch [20/30] Avg Loss: 45.7479\n",
      "Epoch [199/900] completed. Average Loss:  44.1769\n",
      "Epoch [200/900], Batch [20/30] Avg Loss: 38.9539\n",
      "Epoch [200/900] completed. Average Loss:  41.5722\n",
      "Epoch [201/900], Batch [20/30] Avg Loss: 42.5126\n",
      "Epoch [201/900] completed. Average Loss:  41.9296\n",
      "Epoch [202/900], Batch [20/30] Avg Loss: 41.3146\n",
      "Epoch [202/900] completed. Average Loss:  41.6610\n",
      "Epoch [203/900], Batch [20/30] Avg Loss: 37.0855\n",
      "Epoch [203/900] completed. Average Loss:  37.2736\n",
      "Epoch [204/900], Batch [20/30] Avg Loss: 39.9820\n",
      "Epoch [204/900] completed. Average Loss:  38.1838\n",
      "Epoch [205/900], Batch [20/30] Avg Loss: 42.5619\n",
      "Epoch [205/900] completed. Average Loss:  39.8806\n",
      "Epoch [206/900], Batch [20/30] Avg Loss: 40.4865\n",
      "Epoch [206/900] completed. Average Loss:  40.6800\n",
      "Epoch [207/900], Batch [20/30] Avg Loss: 41.8325\n",
      "Epoch [207/900] completed. Average Loss:  42.8384\n",
      "Epoch [208/900], Batch [20/30] Avg Loss: 38.7410\n",
      "Epoch [208/900] completed. Average Loss:  37.9785\n",
      "Epoch [209/900], Batch [20/30] Avg Loss: 38.0124\n",
      "Epoch [209/900] completed. Average Loss:  37.8691\n",
      "Epoch [210/900], Batch [20/30] Avg Loss: 41.1292\n",
      "Epoch [210/900] completed. Average Loss:  38.7635\n",
      "Epoch [211/900], Batch [20/30] Avg Loss: 35.9686\n",
      "Epoch [211/900] completed. Average Loss:  37.8206\n",
      "Epoch [212/900], Batch [20/30] Avg Loss: 37.5816\n",
      "Epoch [212/900] completed. Average Loss:  37.8164\n",
      "Epoch [213/900], Batch [20/30] Avg Loss: 39.8598\n",
      "Epoch [213/900] completed. Average Loss:  39.9937\n",
      "Epoch [214/900], Batch [20/30] Avg Loss: 41.2300\n",
      "Epoch [214/900] completed. Average Loss:  41.7498\n",
      "Epoch [215/900], Batch [20/30] Avg Loss: 37.1534\n",
      "Epoch [215/900] completed. Average Loss:  37.0037\n",
      "Epoch [216/900], Batch [20/30] Avg Loss: 40.5665\n",
      "Epoch [216/900] completed. Average Loss:  39.6573\n",
      "Epoch [217/900], Batch [20/30] Avg Loss: 41.7799\n",
      "Epoch [217/900] completed. Average Loss:  40.4409\n",
      "Epoch [218/900], Batch [20/30] Avg Loss: 44.6099\n",
      "Epoch [218/900] completed. Average Loss:  42.6040\n",
      "Epoch [219/900], Batch [20/30] Avg Loss: 39.4326\n",
      "Epoch [219/900] completed. Average Loss:  39.7510\n",
      "Epoch [220/900], Batch [20/30] Avg Loss: 39.9753\n",
      "Epoch [220/900] completed. Average Loss:  39.8690\n",
      "Epoch [221/900], Batch [20/30] Avg Loss: 43.2933\n",
      "Epoch [221/900] completed. Average Loss:  42.6235\n",
      "Epoch [222/900], Batch [20/30] Avg Loss: 40.1303\n",
      "Epoch [222/900] completed. Average Loss:  41.1613\n",
      "Epoch [223/900], Batch [20/30] Avg Loss: 44.0363\n",
      "Epoch [223/900] completed. Average Loss:  41.5421\n",
      "Epoch [224/900], Batch [20/30] Avg Loss: 41.0281\n",
      "Epoch [224/900] completed. Average Loss:  40.2124\n",
      "Epoch [225/900], Batch [20/30] Avg Loss: 38.6703\n",
      "Epoch [225/900] completed. Average Loss:  39.1107\n",
      "Epoch [226/900], Batch [20/30] Avg Loss: 34.6382\n",
      "Epoch [226/900] completed. Average Loss:  36.7337\n",
      "Epoch [227/900], Batch [20/30] Avg Loss: 42.5629\n",
      "Epoch [227/900] completed. Average Loss:  42.0241\n",
      "Epoch [228/900], Batch [20/30] Avg Loss: 38.5214\n",
      "Epoch [228/900] completed. Average Loss:  38.4927\n",
      "Epoch [229/900], Batch [20/30] Avg Loss: 39.1382\n",
      "Epoch [229/900] completed. Average Loss:  39.5327\n",
      "Epoch [230/900], Batch [20/30] Avg Loss: 35.8594\n",
      "Epoch [230/900] completed. Average Loss:  34.3179\n",
      "Epoch [231/900], Batch [20/30] Avg Loss: 43.4959\n",
      "Epoch [231/900] completed. Average Loss:  44.6695\n",
      "Epoch [232/900], Batch [20/30] Avg Loss: 39.5952\n",
      "Epoch [232/900] completed. Average Loss:  38.6948\n",
      "Epoch [233/900], Batch [20/30] Avg Loss: 35.6219\n",
      "Epoch [233/900] completed. Average Loss:  37.9453\n",
      "Epoch [234/900], Batch [20/30] Avg Loss: 36.4506\n",
      "Epoch [234/900] completed. Average Loss:  37.1852\n",
      "Epoch [235/900], Batch [20/30] Avg Loss: 36.8664\n",
      "Epoch [235/900] completed. Average Loss:  37.1722\n",
      "Epoch [236/900], Batch [20/30] Avg Loss: 38.5509\n",
      "Epoch [236/900] completed. Average Loss:  37.5534\n",
      "Epoch [237/900], Batch [20/30] Avg Loss: 34.8026\n",
      "Epoch [237/900] completed. Average Loss:  36.3214\n",
      "Epoch [238/900], Batch [20/30] Avg Loss: 41.7943\n",
      "Epoch [238/900] completed. Average Loss:  42.8309\n",
      "Epoch [239/900], Batch [20/30] Avg Loss: 40.9567\n",
      "Epoch [239/900] completed. Average Loss:  41.2270\n",
      "Epoch [240/900], Batch [20/30] Avg Loss: 40.4466\n",
      "Epoch [240/900] completed. Average Loss:  41.7941\n",
      "Epoch [241/900], Batch [20/30] Avg Loss: 36.2844\n",
      "Epoch [241/900] completed. Average Loss:  36.9157\n",
      "Epoch [242/900], Batch [20/30] Avg Loss: 44.6101\n",
      "Epoch [242/900] completed. Average Loss:  43.7073\n",
      "Epoch [243/900], Batch [20/30] Avg Loss: 46.1217\n",
      "Epoch [243/900] completed. Average Loss:  45.1441\n",
      "Epoch [244/900], Batch [20/30] Avg Loss: 37.6748\n",
      "Epoch [244/900] completed. Average Loss:  40.0748\n",
      "Epoch [245/900], Batch [20/30] Avg Loss: 41.6178\n",
      "Epoch [245/900] completed. Average Loss:  41.5051\n",
      "Epoch [246/900], Batch [20/30] Avg Loss: 42.7567\n",
      "Epoch [246/900] completed. Average Loss:  39.5401\n",
      "Epoch [247/900], Batch [20/30] Avg Loss: 35.5151\n",
      "Epoch [247/900] completed. Average Loss:  37.2197\n",
      "Epoch [248/900], Batch [20/30] Avg Loss: 41.4675\n",
      "Epoch [248/900] completed. Average Loss:  41.6434\n",
      "Epoch [249/900], Batch [20/30] Avg Loss: 36.7976\n",
      "Epoch [249/900] completed. Average Loss:  37.0794\n",
      "Epoch [250/900], Batch [20/30] Avg Loss: 40.5557\n",
      "Epoch [250/900] completed. Average Loss:  39.1721\n",
      "Epoch [251/900], Batch [20/30] Avg Loss: 39.0224\n",
      "Epoch [251/900] completed. Average Loss:  39.6780\n",
      "Epoch [252/900], Batch [20/30] Avg Loss: 39.1798\n",
      "Epoch [252/900] completed. Average Loss:  37.6303\n",
      "Epoch [253/900], Batch [20/30] Avg Loss: 34.6991\n",
      "Epoch [253/900] completed. Average Loss:  36.8967\n",
      "Epoch [254/900], Batch [20/30] Avg Loss: 39.5345\n",
      "Epoch [254/900] completed. Average Loss:  38.8751\n",
      "Epoch [255/900], Batch [20/30] Avg Loss: 40.2859\n",
      "Epoch [255/900] completed. Average Loss:  38.5496\n",
      "Epoch [256/900], Batch [20/30] Avg Loss: 36.9024\n",
      "Epoch [256/900] completed. Average Loss:  38.1861\n",
      "Epoch [257/900], Batch [20/30] Avg Loss: 34.0273\n",
      "Epoch [257/900] completed. Average Loss:  37.6460\n",
      "Epoch [258/900], Batch [20/30] Avg Loss: 41.4147\n",
      "Epoch [258/900] completed. Average Loss:  40.1343\n",
      "Epoch [259/900], Batch [20/30] Avg Loss: 36.2356\n",
      "Epoch [259/900] completed. Average Loss:  37.8217\n",
      "Epoch [260/900], Batch [20/30] Avg Loss: 42.3916\n",
      "Epoch [260/900] completed. Average Loss:  41.6614\n",
      "Epoch [261/900], Batch [20/30] Avg Loss: 43.3173\n",
      "Epoch [261/900] completed. Average Loss:  41.7136\n",
      "Epoch [262/900], Batch [20/30] Avg Loss: 38.0655\n",
      "Epoch [262/900] completed. Average Loss:  38.9661\n",
      "Epoch [263/900], Batch [20/30] Avg Loss: 41.2162\n",
      "Epoch [263/900] completed. Average Loss:  40.4839\n",
      "Epoch [264/900], Batch [20/30] Avg Loss: 42.7309\n",
      "Epoch [264/900] completed. Average Loss:  40.6810\n",
      "Epoch [265/900], Batch [20/30] Avg Loss: 36.4436\n",
      "Epoch [265/900] completed. Average Loss:  34.1199\n",
      "Epoch [266/900], Batch [20/30] Avg Loss: 40.9722\n",
      "Epoch [266/900] completed. Average Loss:  39.8320\n",
      "Epoch [267/900], Batch [20/30] Avg Loss: 40.5279\n",
      "Epoch [267/900] completed. Average Loss:  42.0599\n",
      "Epoch [268/900], Batch [20/30] Avg Loss: 37.6882\n",
      "Epoch [268/900] completed. Average Loss:  37.7084\n",
      "Epoch [269/900], Batch [20/30] Avg Loss: 40.2574\n",
      "Epoch [269/900] completed. Average Loss:  39.0365\n",
      "Epoch [270/900], Batch [20/30] Avg Loss: 42.3829\n",
      "Epoch [270/900] completed. Average Loss:  44.4618\n",
      "Epoch [271/900], Batch [20/30] Avg Loss: 34.6596\n",
      "Epoch [271/900] completed. Average Loss:  34.7143\n",
      "Epoch [272/900], Batch [20/30] Avg Loss: 39.4328\n",
      "Epoch [272/900] completed. Average Loss:  37.7768\n",
      "Epoch [273/900], Batch [20/30] Avg Loss: 41.9468\n",
      "Epoch [273/900] completed. Average Loss:  44.2752\n",
      "Epoch [274/900], Batch [20/30] Avg Loss: 35.3465\n",
      "Epoch [274/900] completed. Average Loss:  37.5865\n",
      "Epoch [275/900], Batch [20/30] Avg Loss: 39.2508\n",
      "Epoch [275/900] completed. Average Loss:  40.6125\n",
      "Epoch [276/900], Batch [20/30] Avg Loss: 38.4438\n",
      "Epoch [276/900] completed. Average Loss:  39.4221\n",
      "Epoch [277/900], Batch [20/30] Avg Loss: 35.0524\n",
      "Epoch [277/900] completed. Average Loss:  39.6165\n",
      "Epoch [278/900], Batch [20/30] Avg Loss: 42.5488\n",
      "Epoch [278/900] completed. Average Loss:  43.0053\n",
      "Epoch [279/900], Batch [20/30] Avg Loss: 38.5048\n",
      "Epoch [279/900] completed. Average Loss:  37.6361\n",
      "Epoch [280/900], Batch [20/30] Avg Loss: 45.8690\n",
      "Epoch [280/900] completed. Average Loss:  45.5016\n",
      "Epoch [281/900], Batch [20/30] Avg Loss: 36.1954\n",
      "Epoch [281/900] completed. Average Loss:  36.2806\n",
      "Epoch [282/900], Batch [20/30] Avg Loss: 39.3582\n",
      "Epoch [282/900] completed. Average Loss:  39.0253\n",
      "Epoch [283/900], Batch [20/30] Avg Loss: 37.6611\n",
      "Epoch [283/900] completed. Average Loss:  36.0958\n",
      "Epoch [284/900], Batch [20/30] Avg Loss: 37.0427\n",
      "Epoch [284/900] completed. Average Loss:  38.6415\n",
      "Epoch [285/900], Batch [20/30] Avg Loss: 42.1164\n",
      "Epoch [285/900] completed. Average Loss:  41.7012\n",
      "Epoch [286/900], Batch [20/30] Avg Loss: 43.2335\n",
      "Epoch [286/900] completed. Average Loss:  40.3163\n",
      "Epoch [287/900], Batch [20/30] Avg Loss: 38.0277\n",
      "Epoch [287/900] completed. Average Loss:  39.7446\n",
      "Epoch [288/900], Batch [20/30] Avg Loss: 38.4428\n",
      "Epoch [288/900] completed. Average Loss:  38.2670\n",
      "Epoch [289/900], Batch [20/30] Avg Loss: 39.3788\n",
      "Epoch [289/900] completed. Average Loss:  38.9456\n",
      "Epoch [290/900], Batch [20/30] Avg Loss: 34.6171\n",
      "Epoch [290/900] completed. Average Loss:  36.9589\n",
      "Epoch [291/900], Batch [20/30] Avg Loss: 38.3526\n",
      "Epoch [291/900] completed. Average Loss:  36.7239\n",
      "Epoch [292/900], Batch [20/30] Avg Loss: 45.3993\n",
      "Epoch [292/900] completed. Average Loss:  42.8248\n",
      "Epoch [293/900], Batch [20/30] Avg Loss: 40.3558\n",
      "Epoch [293/900] completed. Average Loss:  42.0143\n",
      "Epoch [294/900], Batch [20/30] Avg Loss: 39.5365\n",
      "Epoch [294/900] completed. Average Loss:  39.7704\n",
      "Epoch [295/900], Batch [20/30] Avg Loss: 39.8637\n",
      "Epoch [295/900] completed. Average Loss:  39.8366\n",
      "Epoch [296/900], Batch [20/30] Avg Loss: 38.4157\n",
      "Epoch [296/900] completed. Average Loss:  39.3063\n",
      "Epoch [297/900], Batch [20/30] Avg Loss: 35.3267\n",
      "Epoch [297/900] completed. Average Loss:  34.8335\n",
      "Epoch [298/900], Batch [20/30] Avg Loss: 45.0155\n",
      "Epoch [298/900] completed. Average Loss:  43.1738\n",
      "Epoch [299/900], Batch [20/30] Avg Loss: 41.5617\n",
      "Epoch [299/900] completed. Average Loss:  39.7206\n",
      "Epoch [300/900], Batch [20/30] Avg Loss: 40.9556\n",
      "Epoch [300/900] completed. Average Loss:  42.8306\n",
      "Epoch [301/900], Batch [20/30] Avg Loss: 35.7791\n",
      "Epoch [301/900] completed. Average Loss:  35.0369\n",
      "Epoch [302/900], Batch [20/30] Avg Loss: 39.4122\n",
      "Epoch [302/900] completed. Average Loss:  40.6936\n",
      "Epoch [303/900], Batch [20/30] Avg Loss: 36.1311\n",
      "Epoch [303/900] completed. Average Loss:  35.9112\n",
      "Epoch [304/900], Batch [20/30] Avg Loss: 37.7759\n",
      "Epoch [304/900] completed. Average Loss:  38.7256\n",
      "Epoch [305/900], Batch [20/30] Avg Loss: 38.1110\n",
      "Epoch [305/900] completed. Average Loss:  37.5352\n",
      "Epoch [306/900], Batch [20/30] Avg Loss: 42.4285\n",
      "Epoch [306/900] completed. Average Loss:  39.5180\n",
      "Epoch [307/900], Batch [20/30] Avg Loss: 39.7651\n",
      "Epoch [307/900] completed. Average Loss:  40.7335\n",
      "Epoch [308/900], Batch [20/30] Avg Loss: 41.1548\n",
      "Epoch [308/900] completed. Average Loss:  41.1146\n",
      "Epoch [309/900], Batch [20/30] Avg Loss: 39.1553\n",
      "Epoch [309/900] completed. Average Loss:  37.9390\n",
      "Epoch [310/900], Batch [20/30] Avg Loss: 42.7412\n",
      "Epoch [310/900] completed. Average Loss:  40.4186\n",
      "Epoch [311/900], Batch [20/30] Avg Loss: 38.4696\n",
      "Epoch [311/900] completed. Average Loss:  39.8433\n",
      "Epoch [312/900], Batch [20/30] Avg Loss: 33.7645\n",
      "Epoch [312/900] completed. Average Loss:  34.6460\n",
      "Epoch [313/900], Batch [20/30] Avg Loss: 43.9202\n",
      "Epoch [313/900] completed. Average Loss:  43.1209\n",
      "Epoch [314/900], Batch [20/30] Avg Loss: 38.2882\n",
      "Epoch [314/900] completed. Average Loss:  40.3479\n",
      "Epoch [315/900], Batch [20/30] Avg Loss: 44.1704\n",
      "Epoch [315/900] completed. Average Loss:  44.5293\n",
      "Epoch [316/900], Batch [20/30] Avg Loss: 44.4877\n",
      "Epoch [316/900] completed. Average Loss:  41.0183\n",
      "Epoch [317/900], Batch [20/30] Avg Loss: 36.3445\n",
      "Epoch [317/900] completed. Average Loss:  38.4496\n",
      "Epoch [318/900], Batch [20/30] Avg Loss: 40.8517\n",
      "Epoch [318/900] completed. Average Loss:  41.4463\n",
      "Epoch [319/900], Batch [20/30] Avg Loss: 33.2736\n",
      "Epoch [319/900] completed. Average Loss:  33.5299\n",
      "Epoch [320/900], Batch [20/30] Avg Loss: 43.7791\n",
      "Epoch [320/900] completed. Average Loss:  42.6077\n",
      "Epoch [321/900], Batch [20/30] Avg Loss: 37.4696\n",
      "Epoch [321/900] completed. Average Loss:  37.4009\n",
      "Epoch [322/900], Batch [20/30] Avg Loss: 39.1028\n",
      "Epoch [322/900] completed. Average Loss:  36.8996\n",
      "Epoch [323/900], Batch [20/30] Avg Loss: 40.7681\n",
      "Epoch [323/900] completed. Average Loss:  41.4623\n",
      "Epoch [324/900], Batch [20/30] Avg Loss: 41.6832\n",
      "Epoch [324/900] completed. Average Loss:  40.5121\n",
      "Epoch [325/900], Batch [20/30] Avg Loss: 39.9656\n",
      "Epoch [325/900] completed. Average Loss:  40.0515\n",
      "Epoch [326/900], Batch [20/30] Avg Loss: 39.2391\n",
      "Epoch [326/900] completed. Average Loss:  40.4600\n",
      "Epoch [327/900], Batch [20/30] Avg Loss: 40.9729\n",
      "Epoch [327/900] completed. Average Loss:  40.6188\n",
      "Epoch [328/900], Batch [20/30] Avg Loss: 44.2317\n",
      "Epoch [328/900] completed. Average Loss:  43.8252\n",
      "Epoch [329/900], Batch [20/30] Avg Loss: 39.8382\n",
      "Epoch [329/900] completed. Average Loss:  40.4935\n",
      "Epoch [330/900], Batch [20/30] Avg Loss: 35.2968\n",
      "Epoch [330/900] completed. Average Loss:  35.6756\n",
      "Epoch [331/900], Batch [20/30] Avg Loss: 40.2299\n",
      "Epoch [331/900] completed. Average Loss:  39.8472\n",
      "Epoch [332/900], Batch [20/30] Avg Loss: 40.6993\n",
      "Epoch [332/900] completed. Average Loss:  38.8078\n",
      "Epoch [333/900], Batch [20/30] Avg Loss: 42.1136\n",
      "Epoch [333/900] completed. Average Loss:  41.5356\n",
      "Epoch [334/900], Batch [20/30] Avg Loss: 33.2072\n",
      "Epoch [334/900] completed. Average Loss:  38.0464\n",
      "Epoch [335/900], Batch [20/30] Avg Loss: 42.2536\n",
      "Epoch [335/900] completed. Average Loss:  39.6058\n",
      "Epoch [336/900], Batch [20/30] Avg Loss: 38.3548\n",
      "Epoch [336/900] completed. Average Loss:  36.2190\n",
      "Epoch [337/900], Batch [20/30] Avg Loss: 40.4134\n",
      "Epoch [337/900] completed. Average Loss:  40.4975\n",
      "Epoch [338/900], Batch [20/30] Avg Loss: 39.8337\n",
      "Epoch [338/900] completed. Average Loss:  38.3654\n",
      "Epoch [339/900], Batch [20/30] Avg Loss: 37.1092\n",
      "Epoch [339/900] completed. Average Loss:  37.8337\n",
      "Epoch [340/900], Batch [20/30] Avg Loss: 44.1478\n",
      "Epoch [340/900] completed. Average Loss:  43.8821\n",
      "Epoch [341/900], Batch [20/30] Avg Loss: 35.1512\n",
      "Epoch [341/900] completed. Average Loss:  37.7858\n",
      "Epoch [342/900], Batch [20/30] Avg Loss: 40.7030\n",
      "Epoch [342/900] completed. Average Loss:  41.6190\n",
      "Epoch [343/900], Batch [20/30] Avg Loss: 38.4911\n",
      "Epoch [343/900] completed. Average Loss:  37.4031\n",
      "Epoch [344/900], Batch [20/30] Avg Loss: 38.9649\n",
      "Epoch [344/900] completed. Average Loss:  40.4600\n",
      "Epoch [345/900], Batch [20/30] Avg Loss: 39.4934\n",
      "Epoch [345/900] completed. Average Loss:  39.8179\n",
      "Epoch [346/900], Batch [20/30] Avg Loss: 35.9705\n",
      "Epoch [346/900] completed. Average Loss:  38.7546\n",
      "Epoch [347/900], Batch [20/30] Avg Loss: 37.4021\n",
      "Epoch [347/900] completed. Average Loss:  38.4736\n",
      "Epoch [348/900], Batch [20/30] Avg Loss: 37.6308\n",
      "Epoch [348/900] completed. Average Loss:  37.3539\n",
      "Epoch [349/900], Batch [20/30] Avg Loss: 39.7426\n",
      "Epoch [349/900] completed. Average Loss:  37.9531\n",
      "Epoch [350/900], Batch [20/30] Avg Loss: 32.2204\n",
      "Epoch [350/900] completed. Average Loss:  33.9100\n",
      "Epoch [351/900], Batch [20/30] Avg Loss: 38.8971\n",
      "Epoch [351/900] completed. Average Loss:  39.6559\n",
      "Epoch [352/900], Batch [20/30] Avg Loss: 36.8518\n",
      "Epoch [352/900] completed. Average Loss:  40.6855\n",
      "Epoch [353/900], Batch [20/30] Avg Loss: 38.2679\n",
      "Epoch [353/900] completed. Average Loss:  38.6236\n",
      "Epoch [354/900], Batch [20/30] Avg Loss: 40.8771\n",
      "Epoch [354/900] completed. Average Loss:  41.1737\n",
      "Epoch [355/900], Batch [20/30] Avg Loss: 44.8118\n",
      "Epoch [355/900] completed. Average Loss:  43.1733\n",
      "Epoch [356/900], Batch [20/30] Avg Loss: 37.9043\n",
      "Epoch [356/900] completed. Average Loss:  39.4905\n",
      "Epoch [357/900], Batch [20/30] Avg Loss: 42.0229\n",
      "Epoch [357/900] completed. Average Loss:  39.8406\n",
      "Epoch [358/900], Batch [20/30] Avg Loss: 38.0726\n",
      "Epoch [358/900] completed. Average Loss:  38.4016\n",
      "Epoch [359/900], Batch [20/30] Avg Loss: 37.5567\n",
      "Epoch [359/900] completed. Average Loss:  36.5926\n",
      "Epoch [360/900], Batch [20/30] Avg Loss: 39.6779\n",
      "Epoch [360/900] completed. Average Loss:  39.6043\n",
      "Epoch [361/900], Batch [20/30] Avg Loss: 36.9199\n",
      "Epoch [361/900] completed. Average Loss:  35.9609\n",
      "Epoch [362/900], Batch [20/30] Avg Loss: 40.0014\n",
      "Epoch [362/900] completed. Average Loss:  39.7678\n",
      "Epoch [363/900], Batch [20/30] Avg Loss: 39.2263\n",
      "Epoch [363/900] completed. Average Loss:  40.0912\n",
      "Epoch [364/900], Batch [20/30] Avg Loss: 40.6646\n",
      "Epoch [364/900] completed. Average Loss:  41.1094\n",
      "Epoch [365/900], Batch [20/30] Avg Loss: 36.8010\n",
      "Epoch [365/900] completed. Average Loss:  37.0286\n",
      "Epoch [366/900], Batch [20/30] Avg Loss: 45.9111\n",
      "Epoch [366/900] completed. Average Loss:  44.1770\n",
      "Epoch [367/900], Batch [20/30] Avg Loss: 43.4032\n",
      "Epoch [367/900] completed. Average Loss:  44.1886\n",
      "Epoch [368/900], Batch [20/30] Avg Loss: 34.3153\n",
      "Epoch [368/900] completed. Average Loss:  36.8859\n",
      "Epoch [369/900], Batch [20/30] Avg Loss: 41.7839\n",
      "Epoch [369/900] completed. Average Loss:  40.4480\n",
      "Epoch [370/900], Batch [20/30] Avg Loss: 36.7684\n",
      "Epoch [370/900] completed. Average Loss:  35.7882\n",
      "Epoch [371/900], Batch [20/30] Avg Loss: 37.9836\n",
      "Epoch [371/900] completed. Average Loss:  38.6004\n",
      "Epoch [372/900], Batch [20/30] Avg Loss: 36.8050\n",
      "Epoch [372/900] completed. Average Loss:  37.6733\n",
      "Epoch [373/900], Batch [20/30] Avg Loss: 39.7483\n",
      "Epoch [373/900] completed. Average Loss:  37.5006\n",
      "Epoch [374/900], Batch [20/30] Avg Loss: 37.4765\n",
      "Epoch [374/900] completed. Average Loss:  37.0308\n",
      "Epoch [375/900], Batch [20/30] Avg Loss: 37.3811\n",
      "Epoch [375/900] completed. Average Loss:  37.3100\n",
      "Epoch [376/900], Batch [20/30] Avg Loss: 35.5580\n",
      "Epoch [376/900] completed. Average Loss:  38.1386\n",
      "Epoch [377/900], Batch [20/30] Avg Loss: 36.2430\n",
      "Epoch [377/900] completed. Average Loss:  40.2233\n",
      "Epoch [378/900], Batch [20/30] Avg Loss: 38.0443\n",
      "Epoch [378/900] completed. Average Loss:  40.3014\n",
      "Epoch [379/900], Batch [20/30] Avg Loss: 38.1037\n",
      "Epoch [379/900] completed. Average Loss:  36.1847\n",
      "Epoch [380/900], Batch [20/30] Avg Loss: 37.0271\n",
      "Epoch [380/900] completed. Average Loss:  37.2352\n",
      "Epoch [381/900], Batch [20/30] Avg Loss: 38.4617\n",
      "Epoch [381/900] completed. Average Loss:  39.5385\n",
      "Epoch [382/900], Batch [20/30] Avg Loss: 39.1118\n",
      "Epoch [382/900] completed. Average Loss:  37.2261\n",
      "Epoch [383/900], Batch [20/30] Avg Loss: 40.7826\n",
      "Epoch [383/900] completed. Average Loss:  37.8295\n",
      "Epoch [384/900], Batch [20/30] Avg Loss: 41.1515\n",
      "Epoch [384/900] completed. Average Loss:  40.4299\n",
      "Epoch [385/900], Batch [20/30] Avg Loss: 40.8164\n",
      "Epoch [385/900] completed. Average Loss:  40.8515\n",
      "Epoch [386/900], Batch [20/30] Avg Loss: 39.0216\n",
      "Epoch [386/900] completed. Average Loss:  36.9083\n",
      "Epoch [387/900], Batch [20/30] Avg Loss: 37.6018\n",
      "Epoch [387/900] completed. Average Loss:  36.7190\n",
      "Epoch [388/900], Batch [20/30] Avg Loss: 43.5396\n",
      "Epoch [388/900] completed. Average Loss:  42.4836\n",
      "Epoch [389/900], Batch [20/30] Avg Loss: 41.4363\n",
      "Epoch [389/900] completed. Average Loss:  40.4885\n",
      "Epoch [390/900], Batch [20/30] Avg Loss: 41.1060\n",
      "Epoch [390/900] completed. Average Loss:  41.4892\n",
      "Epoch [391/900], Batch [20/30] Avg Loss: 35.5276\n",
      "Epoch [391/900] completed. Average Loss:  34.8530\n",
      "Epoch [392/900], Batch [20/30] Avg Loss: 44.3010\n",
      "Epoch [392/900] completed. Average Loss:  41.9416\n",
      "Epoch [393/900], Batch [20/30] Avg Loss: 39.9764\n",
      "Epoch [393/900] completed. Average Loss:  40.4445\n",
      "Epoch [394/900], Batch [20/30] Avg Loss: 36.1293\n",
      "Epoch [394/900] completed. Average Loss:  36.2143\n",
      "Epoch [395/900], Batch [20/30] Avg Loss: 35.1031\n",
      "Epoch [395/900] completed. Average Loss:  36.2239\n",
      "Epoch [396/900], Batch [20/30] Avg Loss: 45.4649\n",
      "Epoch [396/900] completed. Average Loss:  40.4157\n",
      "Epoch [397/900], Batch [20/30] Avg Loss: 36.2415\n",
      "Epoch [397/900] completed. Average Loss:  38.3401\n",
      "Epoch [398/900], Batch [20/30] Avg Loss: 39.5214\n",
      "Epoch [398/900] completed. Average Loss:  39.0427\n",
      "Epoch [399/900], Batch [20/30] Avg Loss: 37.7185\n",
      "Epoch [399/900] completed. Average Loss:  38.8490\n",
      "Epoch [400/900], Batch [20/30] Avg Loss: 37.8454\n",
      "Epoch [400/900] completed. Average Loss:  36.5095\n",
      "Epoch [401/900], Batch [20/30] Avg Loss: 38.4545\n",
      "Epoch [401/900] completed. Average Loss:  41.1134\n",
      "Epoch [402/900], Batch [20/30] Avg Loss: 39.2860\n",
      "Epoch [402/900] completed. Average Loss:  43.6124\n",
      "Epoch [403/900], Batch [20/30] Avg Loss: 39.0920\n",
      "Epoch [403/900] completed. Average Loss:  37.9772\n",
      "Epoch [404/900], Batch [20/30] Avg Loss: 37.3827\n",
      "Epoch [404/900] completed. Average Loss:  35.4245\n",
      "Epoch [405/900], Batch [20/30] Avg Loss: 41.0304\n",
      "Epoch [405/900] completed. Average Loss:  38.7438\n",
      "Epoch [406/900], Batch [20/30] Avg Loss: 41.1218\n",
      "Epoch [406/900] completed. Average Loss:  39.0110\n",
      "Epoch [407/900], Batch [20/30] Avg Loss: 38.1574\n",
      "Epoch [407/900] completed. Average Loss:  40.8414\n",
      "Epoch [408/900], Batch [20/30] Avg Loss: 38.6211\n",
      "Epoch [408/900] completed. Average Loss:  37.9659\n",
      "Epoch [409/900], Batch [20/30] Avg Loss: 40.6269\n",
      "Epoch [409/900] completed. Average Loss:  41.6663\n",
      "Epoch [410/900], Batch [20/30] Avg Loss: 38.2459\n",
      "Epoch [410/900] completed. Average Loss:  38.7520\n",
      "Epoch [411/900], Batch [20/30] Avg Loss: 40.1255\n",
      "Epoch [411/900] completed. Average Loss:  41.4517\n",
      "Epoch [412/900], Batch [20/30] Avg Loss: 35.7307\n",
      "Epoch [412/900] completed. Average Loss:  37.4937\n",
      "Epoch [413/900], Batch [20/30] Avg Loss: 37.3291\n",
      "Epoch [413/900] completed. Average Loss:  37.3560\n",
      "Epoch [414/900], Batch [20/30] Avg Loss: 34.8998\n",
      "Epoch [414/900] completed. Average Loss:  37.3890\n",
      "Epoch [415/900], Batch [20/30] Avg Loss: 38.0634\n",
      "Epoch [415/900] completed. Average Loss:  37.3360\n",
      "Epoch [416/900], Batch [20/30] Avg Loss: 35.0164\n",
      "Epoch [416/900] completed. Average Loss:  34.0949\n",
      "Epoch [417/900], Batch [20/30] Avg Loss: 43.9788\n",
      "Epoch [417/900] completed. Average Loss:  41.2118\n",
      "Epoch [418/900], Batch [20/30] Avg Loss: 40.8813\n",
      "Epoch [418/900] completed. Average Loss:  41.2803\n",
      "Epoch [419/900], Batch [20/30] Avg Loss: 50.4051\n",
      "Epoch [419/900] completed. Average Loss:  51.1178\n",
      "Epoch [420/900], Batch [20/30] Avg Loss: 38.1913\n",
      "Epoch [420/900] completed. Average Loss:  38.7734\n",
      "Epoch [421/900], Batch [20/30] Avg Loss: 38.2268\n",
      "Epoch [421/900] completed. Average Loss:  39.1697\n",
      "Epoch [422/900], Batch [20/30] Avg Loss: 40.5468\n",
      "Epoch [422/900] completed. Average Loss:  42.2188\n",
      "Epoch [423/900], Batch [20/30] Avg Loss: 37.7835\n",
      "Epoch [423/900] completed. Average Loss:  39.9918\n",
      "Epoch [424/900], Batch [20/30] Avg Loss: 40.6749\n",
      "Epoch [424/900] completed. Average Loss:  38.8241\n",
      "Epoch [425/900], Batch [20/30] Avg Loss: 35.3890\n",
      "Epoch [425/900] completed. Average Loss:  33.6569\n",
      "Epoch [426/900], Batch [20/30] Avg Loss: 38.7387\n",
      "Epoch [426/900] completed. Average Loss:  40.6420\n",
      "Epoch [427/900], Batch [20/30] Avg Loss: 38.8399\n",
      "Epoch [427/900] completed. Average Loss:  38.7145\n",
      "Epoch [428/900], Batch [20/30] Avg Loss: 42.2705\n",
      "Epoch [428/900] completed. Average Loss:  42.8172\n",
      "Epoch [429/900], Batch [20/30] Avg Loss: 37.4674\n",
      "Epoch [429/900] completed. Average Loss:  37.2360\n",
      "Epoch [430/900], Batch [20/30] Avg Loss: 35.7468\n",
      "Epoch [430/900] completed. Average Loss:  35.5930\n",
      "Epoch [431/900], Batch [20/30] Avg Loss: 38.5602\n",
      "Epoch [431/900] completed. Average Loss:  38.5715\n",
      "Epoch [432/900], Batch [20/30] Avg Loss: 42.0047\n",
      "Epoch [432/900] completed. Average Loss:  42.8341\n",
      "Epoch [433/900], Batch [20/30] Avg Loss: 38.2514\n",
      "Epoch [433/900] completed. Average Loss:  38.1582\n",
      "Epoch [434/900], Batch [20/30] Avg Loss: 38.0735\n",
      "Epoch [434/900] completed. Average Loss:  35.9824\n",
      "Epoch [435/900], Batch [20/30] Avg Loss: 36.1385\n",
      "Epoch [435/900] completed. Average Loss:  35.9761\n",
      "Epoch [436/900], Batch [20/30] Avg Loss: 40.4683\n",
      "Epoch [436/900] completed. Average Loss:  40.6357\n",
      "Epoch [437/900], Batch [20/30] Avg Loss: 42.4201\n",
      "Epoch [437/900] completed. Average Loss:  41.1422\n",
      "Epoch [438/900], Batch [20/30] Avg Loss: 43.3222\n",
      "Epoch [438/900] completed. Average Loss:  42.6547\n",
      "Epoch [439/900], Batch [20/30] Avg Loss: 35.8463\n",
      "Epoch [439/900] completed. Average Loss:  38.4148\n",
      "Epoch [440/900], Batch [20/30] Avg Loss: 46.8477\n",
      "Epoch [440/900] completed. Average Loss:  43.5802\n",
      "Epoch [441/900], Batch [20/30] Avg Loss: 42.6352\n",
      "Epoch [441/900] completed. Average Loss:  41.1336\n",
      "Epoch [442/900], Batch [20/30] Avg Loss: 37.2260\n",
      "Epoch [442/900] completed. Average Loss:  37.6255\n",
      "Epoch [443/900], Batch [20/30] Avg Loss: 36.9378\n",
      "Epoch [443/900] completed. Average Loss:  38.5756\n",
      "Epoch [444/900], Batch [20/30] Avg Loss: 35.1842\n",
      "Epoch [444/900] completed. Average Loss:  36.6987\n",
      "Epoch [445/900], Batch [20/30] Avg Loss: 39.8714\n",
      "Epoch [445/900] completed. Average Loss:  39.0080\n",
      "Epoch [446/900], Batch [20/30] Avg Loss: 39.2637\n",
      "Epoch [446/900] completed. Average Loss:  40.5090\n",
      "Epoch [447/900], Batch [20/30] Avg Loss: 39.4897\n",
      "Epoch [447/900] completed. Average Loss:  41.0885\n",
      "Epoch [448/900], Batch [20/30] Avg Loss: 37.5686\n",
      "Epoch [448/900] completed. Average Loss:  39.6276\n",
      "Epoch [449/900], Batch [20/30] Avg Loss: 37.7260\n",
      "Epoch [449/900] completed. Average Loss:  38.8693\n",
      "Epoch [450/900], Batch [20/30] Avg Loss: 34.0498\n",
      "Epoch [450/900] completed. Average Loss:  37.5429\n",
      "Epoch [451/900], Batch [20/30] Avg Loss: 34.2809\n",
      "Epoch [451/900] completed. Average Loss:  34.8591\n",
      "Epoch [452/900], Batch [20/30] Avg Loss: 43.1875\n",
      "Epoch [452/900] completed. Average Loss:  41.8304\n",
      "Epoch [453/900], Batch [20/30] Avg Loss: 38.4706\n",
      "Epoch [453/900] completed. Average Loss:  35.8969\n",
      "Epoch [454/900], Batch [20/30] Avg Loss: 41.2950\n",
      "Epoch [454/900] completed. Average Loss:  39.0127\n",
      "Epoch [455/900], Batch [20/30] Avg Loss: 42.3799\n",
      "Epoch [455/900] completed. Average Loss:  40.4104\n",
      "Epoch [456/900], Batch [20/30] Avg Loss: 41.4007\n",
      "Epoch [456/900] completed. Average Loss:  40.2554\n",
      "Epoch [457/900], Batch [20/30] Avg Loss: 33.3454\n",
      "Epoch [457/900] completed. Average Loss:  34.0292\n",
      "Epoch [458/900], Batch [20/30] Avg Loss: 38.9958\n",
      "Epoch [458/900] completed. Average Loss:  37.9653\n",
      "Epoch [459/900], Batch [20/30] Avg Loss: 37.0153\n",
      "Epoch [459/900] completed. Average Loss:  38.9366\n",
      "Epoch [460/900], Batch [20/30] Avg Loss: 37.3113\n",
      "Epoch [460/900] completed. Average Loss:  36.9611\n",
      "Epoch [461/900], Batch [20/30] Avg Loss: 37.5299\n",
      "Epoch [461/900] completed. Average Loss:  37.8478\n",
      "Epoch [462/900], Batch [20/30] Avg Loss: 38.6867\n",
      "Epoch [462/900] completed. Average Loss:  37.6064\n",
      "Epoch [463/900], Batch [20/30] Avg Loss: 34.3942\n",
      "Epoch [463/900] completed. Average Loss:  36.7407\n",
      "Epoch [464/900], Batch [20/30] Avg Loss: 34.5485\n",
      "Epoch [464/900] completed. Average Loss:  35.7612\n",
      "Epoch [465/900], Batch [20/30] Avg Loss: 40.4218\n",
      "Epoch [465/900] completed. Average Loss:  39.5613\n",
      "Epoch [466/900], Batch [20/30] Avg Loss: 37.5633\n",
      "Epoch [466/900] completed. Average Loss:  37.3387\n",
      "Epoch [467/900], Batch [20/30] Avg Loss: 37.1429\n",
      "Epoch [467/900] completed. Average Loss:  42.0181\n",
      "Epoch [468/900], Batch [20/30] Avg Loss: 35.3853\n",
      "Epoch [468/900] completed. Average Loss:  36.3277\n",
      "Epoch [469/900], Batch [20/30] Avg Loss: 39.2873\n",
      "Epoch [469/900] completed. Average Loss:  37.9107\n",
      "Epoch [470/900], Batch [20/30] Avg Loss: 40.9536\n",
      "Epoch [470/900] completed. Average Loss:  39.4005\n",
      "Epoch [471/900], Batch [20/30] Avg Loss: 31.8664\n",
      "Epoch [471/900] completed. Average Loss:  35.2223\n",
      "Epoch [472/900], Batch [20/30] Avg Loss: 39.3386\n",
      "Epoch [472/900] completed. Average Loss:  38.3425\n",
      "Epoch [473/900], Batch [20/30] Avg Loss: 35.2894\n",
      "Epoch [473/900] completed. Average Loss:  38.6671\n",
      "Epoch [474/900], Batch [20/30] Avg Loss: 33.9550\n",
      "Epoch [474/900] completed. Average Loss:  35.3395\n",
      "Epoch [475/900], Batch [20/30] Avg Loss: 39.1530\n",
      "Epoch [475/900] completed. Average Loss:  39.6502\n",
      "Epoch [476/900], Batch [20/30] Avg Loss: 45.9746\n",
      "Epoch [476/900] completed. Average Loss:  42.1607\n",
      "Epoch [477/900], Batch [20/30] Avg Loss: 36.4116\n",
      "Epoch [477/900] completed. Average Loss:  35.5986\n",
      "Epoch [478/900], Batch [20/30] Avg Loss: 40.7195\n",
      "Epoch [478/900] completed. Average Loss:  42.2304\n",
      "Epoch [479/900], Batch [20/30] Avg Loss: 42.8374\n",
      "Epoch [479/900] completed. Average Loss:  42.2120\n",
      "Epoch [480/900], Batch [20/30] Avg Loss: 34.4320\n",
      "Epoch [480/900] completed. Average Loss:  37.2144\n",
      "Epoch [481/900], Batch [20/30] Avg Loss: 37.4368\n",
      "Epoch [481/900] completed. Average Loss:  36.9518\n",
      "Epoch [482/900], Batch [20/30] Avg Loss: 40.2829\n",
      "Epoch [482/900] completed. Average Loss:  38.6919\n",
      "Epoch [483/900], Batch [20/30] Avg Loss: 37.5363\n",
      "Epoch [483/900] completed. Average Loss:  37.2759\n",
      "Epoch [484/900], Batch [20/30] Avg Loss: 33.8248\n",
      "Epoch [484/900] completed. Average Loss:  39.1523\n",
      "Epoch [485/900], Batch [20/30] Avg Loss: 38.2372\n",
      "Epoch [485/900] completed. Average Loss:  39.0115\n",
      "Epoch [486/900], Batch [20/30] Avg Loss: 36.6671\n",
      "Epoch [486/900] completed. Average Loss:  37.9363\n",
      "Epoch [487/900], Batch [20/30] Avg Loss: 40.9829\n",
      "Epoch [487/900] completed. Average Loss:  39.8740\n",
      "Epoch [488/900], Batch [20/30] Avg Loss: 43.5540\n",
      "Epoch [488/900] completed. Average Loss:  41.2243\n",
      "Epoch [489/900], Batch [20/30] Avg Loss: 39.2953\n",
      "Epoch [489/900] completed. Average Loss:  38.5124\n",
      "Epoch [490/900], Batch [20/30] Avg Loss: 39.3599\n",
      "Epoch [490/900] completed. Average Loss:  39.4099\n",
      "Epoch [491/900], Batch [20/30] Avg Loss: 42.7551\n",
      "Epoch [491/900] completed. Average Loss:  42.1385\n",
      "Epoch [492/900], Batch [20/30] Avg Loss: 37.9911\n",
      "Epoch [492/900] completed. Average Loss:  38.1055\n",
      "Epoch [493/900], Batch [20/30] Avg Loss: 36.5366\n",
      "Epoch [493/900] completed. Average Loss:  36.2199\n",
      "Epoch [494/900], Batch [20/30] Avg Loss: 38.8570\n",
      "Epoch [494/900] completed. Average Loss:  40.7589\n",
      "Epoch [495/900], Batch [20/30] Avg Loss: 40.4395\n",
      "Epoch [495/900] completed. Average Loss:  39.4359\n",
      "Epoch [496/900], Batch [20/30] Avg Loss: 41.4314\n",
      "Epoch [496/900] completed. Average Loss:  39.7031\n",
      "Epoch [497/900], Batch [20/30] Avg Loss: 37.7597\n",
      "Epoch [497/900] completed. Average Loss:  38.6941\n",
      "Epoch [498/900], Batch [20/30] Avg Loss: 41.6646\n",
      "Epoch [498/900] completed. Average Loss:  40.1881\n",
      "Epoch [499/900], Batch [20/30] Avg Loss: 45.4424\n",
      "Epoch [499/900] completed. Average Loss:  43.0028\n",
      "Epoch [500/900], Batch [20/30] Avg Loss: 38.3217\n",
      "Epoch [500/900] completed. Average Loss:  36.8857\n",
      "Epoch [501/900], Batch [20/30] Avg Loss: 36.7216\n",
      "Epoch [501/900] completed. Average Loss:  38.8492\n",
      "Epoch [502/900], Batch [20/30] Avg Loss: 38.2385\n",
      "Epoch [502/900] completed. Average Loss:  36.7615\n",
      "Epoch [503/900], Batch [20/30] Avg Loss: 38.0090\n",
      "Epoch [503/900] completed. Average Loss:  38.4951\n",
      "Epoch [504/900], Batch [20/30] Avg Loss: 37.9645\n",
      "Epoch [504/900] completed. Average Loss:  38.1006\n",
      "Epoch [505/900], Batch [20/30] Avg Loss: 37.1188\n",
      "Epoch [505/900] completed. Average Loss:  39.2662\n",
      "Epoch [506/900], Batch [20/30] Avg Loss: 30.9035\n",
      "Epoch [506/900] completed. Average Loss:  31.5332\n",
      "Epoch [507/900], Batch [20/30] Avg Loss: 40.6452\n",
      "Epoch [507/900] completed. Average Loss:  39.2193\n",
      "Epoch [508/900], Batch [20/30] Avg Loss: 41.0854\n",
      "Epoch [508/900] completed. Average Loss:  41.7464\n",
      "Epoch [509/900], Batch [20/30] Avg Loss: 41.3758\n",
      "Epoch [509/900] completed. Average Loss:  40.7519\n",
      "Epoch [510/900], Batch [20/30] Avg Loss: 38.5535\n",
      "Epoch [510/900] completed. Average Loss:  37.5004\n",
      "Epoch [511/900], Batch [20/30] Avg Loss: 32.8000\n",
      "Epoch [511/900] completed. Average Loss:  34.0504\n",
      "Epoch [512/900], Batch [20/30] Avg Loss: 41.5575\n",
      "Epoch [512/900] completed. Average Loss:  41.2392\n",
      "Epoch [513/900], Batch [20/30] Avg Loss: 46.8793\n",
      "Epoch [513/900] completed. Average Loss:  43.5357\n",
      "Epoch [514/900], Batch [20/30] Avg Loss: 35.5710\n",
      "Epoch [514/900] completed. Average Loss:  35.8134\n",
      "Epoch [515/900], Batch [20/30] Avg Loss: 38.5561\n",
      "Epoch [515/900] completed. Average Loss:  38.5121\n",
      "Epoch [516/900], Batch [20/30] Avg Loss: 38.8681\n",
      "Epoch [516/900] completed. Average Loss:  39.7806\n",
      "Epoch [517/900], Batch [20/30] Avg Loss: 34.3269\n",
      "Epoch [517/900] completed. Average Loss:  35.9155\n",
      "Epoch [518/900], Batch [20/30] Avg Loss: 38.3721\n",
      "Epoch [518/900] completed. Average Loss:  38.3584\n",
      "Epoch [519/900], Batch [20/30] Avg Loss: 40.5442\n",
      "Epoch [519/900] completed. Average Loss:  39.7464\n",
      "Epoch [520/900], Batch [20/30] Avg Loss: 33.0547\n",
      "Epoch [520/900] completed. Average Loss:  34.7347\n",
      "Epoch [521/900], Batch [20/30] Avg Loss: 42.8750\n",
      "Epoch [521/900] completed. Average Loss:  39.8633\n",
      "Epoch [522/900], Batch [20/30] Avg Loss: 37.4829\n",
      "Epoch [522/900] completed. Average Loss:  37.1955\n",
      "Epoch [523/900], Batch [20/30] Avg Loss: 37.1470\n",
      "Epoch [523/900] completed. Average Loss:  34.7811\n",
      "Epoch [524/900], Batch [20/30] Avg Loss: 37.3941\n",
      "Epoch [524/900] completed. Average Loss:  36.9950\n",
      "Epoch [525/900], Batch [20/30] Avg Loss: 36.3365\n",
      "Epoch [525/900] completed. Average Loss:  36.8592\n",
      "Epoch [526/900], Batch [20/30] Avg Loss: 36.5677\n",
      "Epoch [526/900] completed. Average Loss:  35.7471\n",
      "Epoch [527/900], Batch [20/30] Avg Loss: 37.2228\n",
      "Epoch [527/900] completed. Average Loss:  38.8693\n",
      "Epoch [528/900], Batch [20/30] Avg Loss: 38.7339\n",
      "Epoch [528/900] completed. Average Loss:  39.5852\n",
      "Epoch [529/900], Batch [20/30] Avg Loss: 37.2896\n",
      "Epoch [529/900] completed. Average Loss:  39.9360\n",
      "Epoch [530/900], Batch [20/30] Avg Loss: 43.6308\n",
      "Epoch [530/900] completed. Average Loss:  39.2777\n",
      "Epoch [531/900], Batch [20/30] Avg Loss: 38.6405\n",
      "Epoch [531/900] completed. Average Loss:  40.9765\n",
      "Epoch [532/900], Batch [20/30] Avg Loss: 37.8342\n",
      "Epoch [532/900] completed. Average Loss:  39.5011\n",
      "Epoch [533/900], Batch [20/30] Avg Loss: 37.9164\n",
      "Epoch [533/900] completed. Average Loss:  35.3906\n",
      "Epoch [534/900], Batch [20/30] Avg Loss: 35.9692\n",
      "Epoch [534/900] completed. Average Loss:  35.9329\n",
      "Epoch [535/900], Batch [20/30] Avg Loss: 38.3499\n",
      "Epoch [535/900] completed. Average Loss:  38.4468\n",
      "Epoch [536/900], Batch [20/30] Avg Loss: 41.5156\n",
      "Epoch [536/900] completed. Average Loss:  39.8385\n",
      "Epoch [537/900], Batch [20/30] Avg Loss: 39.5574\n",
      "Epoch [537/900] completed. Average Loss:  39.6013\n",
      "Epoch [538/900], Batch [20/30] Avg Loss: 42.6316\n",
      "Epoch [538/900] completed. Average Loss:  40.0634\n",
      "Epoch [539/900], Batch [20/30] Avg Loss: 32.5784\n",
      "Epoch [539/900] completed. Average Loss:  33.7741\n",
      "Epoch [540/900], Batch [20/30] Avg Loss: 39.6398\n",
      "Epoch [540/900] completed. Average Loss:  38.9186\n",
      "Epoch [541/900], Batch [20/30] Avg Loss: 42.0882\n",
      "Epoch [541/900] completed. Average Loss:  40.8138\n",
      "Epoch [542/900], Batch [20/30] Avg Loss: 35.5727\n",
      "Epoch [542/900] completed. Average Loss:  35.3801\n",
      "Epoch [543/900], Batch [20/30] Avg Loss: 38.7336\n",
      "Epoch [543/900] completed. Average Loss:  41.4369\n",
      "Epoch [544/900], Batch [20/30] Avg Loss: 36.6068\n",
      "Epoch [544/900] completed. Average Loss:  36.2669\n",
      "Epoch [545/900], Batch [20/30] Avg Loss: 43.2441\n",
      "Epoch [545/900] completed. Average Loss:  42.1733\n",
      "Epoch [546/900], Batch [20/30] Avg Loss: 37.4617\n",
      "Epoch [546/900] completed. Average Loss:  35.5052\n",
      "Epoch [547/900], Batch [20/30] Avg Loss: 38.1273\n",
      "Epoch [547/900] completed. Average Loss:  39.6327\n",
      "Epoch [548/900], Batch [20/30] Avg Loss: 40.7059\n",
      "Epoch [548/900] completed. Average Loss:  41.5304\n",
      "Epoch [549/900], Batch [20/30] Avg Loss: 34.5319\n",
      "Epoch [549/900] completed. Average Loss:  34.7440\n",
      "Epoch [550/900], Batch [20/30] Avg Loss: 38.1826\n",
      "Epoch [550/900] completed. Average Loss:  36.5442\n",
      "Epoch [551/900], Batch [20/30] Avg Loss: 37.2473\n",
      "Epoch [551/900] completed. Average Loss:  36.6453\n",
      "Epoch [552/900], Batch [20/30] Avg Loss: 39.6860\n",
      "Epoch [552/900] completed. Average Loss:  38.4934\n",
      "Epoch [553/900], Batch [20/30] Avg Loss: 40.5567\n",
      "Epoch [553/900] completed. Average Loss:  41.1271\n",
      "Epoch [554/900], Batch [20/30] Avg Loss: 38.5830\n",
      "Epoch [554/900] completed. Average Loss:  38.4384\n",
      "Epoch [555/900], Batch [20/30] Avg Loss: 34.1747\n",
      "Epoch [555/900] completed. Average Loss:  34.1234\n",
      "Epoch [556/900], Batch [20/30] Avg Loss: 38.2185\n",
      "Epoch [556/900] completed. Average Loss:  37.1245\n",
      "Epoch [557/900], Batch [20/30] Avg Loss: 36.2162\n",
      "Epoch [557/900] completed. Average Loss:  35.1991\n",
      "Epoch [558/900], Batch [20/30] Avg Loss: 35.0439\n",
      "Epoch [558/900] completed. Average Loss:  34.5573\n",
      "Epoch [559/900], Batch [20/30] Avg Loss: 37.5159\n",
      "Epoch [559/900] completed. Average Loss:  36.2679\n",
      "Epoch [560/900], Batch [20/30] Avg Loss: 35.6247\n",
      "Epoch [560/900] completed. Average Loss:  34.2982\n",
      "Epoch [561/900], Batch [20/30] Avg Loss: 35.6370\n",
      "Epoch [561/900] completed. Average Loss:  36.4670\n",
      "Epoch [562/900], Batch [20/30] Avg Loss: 37.4672\n",
      "Epoch [562/900] completed. Average Loss:  38.9323\n",
      "Epoch [563/900], Batch [20/30] Avg Loss: 38.7611\n",
      "Epoch [563/900] completed. Average Loss:  37.2679\n",
      "Epoch [564/900], Batch [20/30] Avg Loss: 44.6728\n",
      "Epoch [564/900] completed. Average Loss:  40.9497\n",
      "Epoch [565/900], Batch [20/30] Avg Loss: 42.6593\n",
      "Epoch [565/900] completed. Average Loss:  42.2999\n",
      "Epoch [566/900], Batch [20/30] Avg Loss: 38.3999\n",
      "Epoch [566/900] completed. Average Loss:  39.5124\n",
      "Epoch [567/900], Batch [20/30] Avg Loss: 41.0278\n",
      "Epoch [567/900] completed. Average Loss:  38.8577\n",
      "Epoch [568/900], Batch [20/30] Avg Loss: 42.6764\n",
      "Epoch [568/900] completed. Average Loss:  39.9778\n",
      "Epoch [569/900], Batch [20/30] Avg Loss: 34.1639\n",
      "Epoch [569/900] completed. Average Loss:  34.2093\n",
      "Epoch [570/900], Batch [20/30] Avg Loss: 40.0757\n",
      "Epoch [570/900] completed. Average Loss:  39.3559\n",
      "Epoch [571/900], Batch [20/30] Avg Loss: 40.4437\n",
      "Epoch [571/900] completed. Average Loss:  37.9688\n",
      "Epoch [572/900], Batch [20/30] Avg Loss: 32.5992\n",
      "Epoch [572/900] completed. Average Loss:  34.8441\n",
      "Epoch [573/900], Batch [20/30] Avg Loss: 34.8020\n",
      "Epoch [573/900] completed. Average Loss:  36.7783\n",
      "Epoch [574/900], Batch [20/30] Avg Loss: 40.3510\n",
      "Epoch [574/900] completed. Average Loss:  39.5095\n",
      "Epoch [575/900], Batch [20/30] Avg Loss: 46.2394\n",
      "Epoch [575/900] completed. Average Loss:  43.8671\n",
      "Epoch [576/900], Batch [20/30] Avg Loss: 40.7530\n",
      "Epoch [576/900] completed. Average Loss:  43.5044\n",
      "Epoch [577/900], Batch [20/30] Avg Loss: 41.7200\n",
      "Epoch [577/900] completed. Average Loss:  39.7378\n",
      "Epoch [578/900], Batch [20/30] Avg Loss: 38.6047\n",
      "Epoch [578/900] completed. Average Loss:  37.4894\n",
      "Epoch [579/900], Batch [20/30] Avg Loss: 35.5153\n",
      "Epoch [579/900] completed. Average Loss:  36.6143\n",
      "Epoch [580/900], Batch [20/30] Avg Loss: 37.7200\n",
      "Epoch [580/900] completed. Average Loss:  37.0541\n",
      "Epoch [581/900], Batch [20/30] Avg Loss: 38.7103\n",
      "Epoch [581/900] completed. Average Loss:  39.5716\n",
      "Epoch [582/900], Batch [20/30] Avg Loss: 37.1489\n",
      "Epoch [582/900] completed. Average Loss:  37.4582\n",
      "Epoch [583/900], Batch [20/30] Avg Loss: 33.1674\n",
      "Epoch [583/900] completed. Average Loss:  36.3485\n",
      "Epoch [584/900], Batch [20/30] Avg Loss: 39.7759\n",
      "Epoch [584/900] completed. Average Loss:  37.4050\n",
      "Epoch [585/900], Batch [20/30] Avg Loss: 41.6611\n",
      "Epoch [585/900] completed. Average Loss:  38.8680\n",
      "Epoch [586/900], Batch [20/30] Avg Loss: 38.0457\n",
      "Epoch [586/900] completed. Average Loss:  37.1014\n",
      "Epoch [587/900], Batch [20/30] Avg Loss: 41.0610\n",
      "Epoch [587/900] completed. Average Loss:  40.9385\n",
      "Epoch [588/900], Batch [20/30] Avg Loss: 37.1908\n",
      "Epoch [588/900] completed. Average Loss:  38.5099\n",
      "Epoch [589/900], Batch [20/30] Avg Loss: 36.8115\n",
      "Epoch [589/900] completed. Average Loss:  39.8516\n",
      "Epoch [590/900], Batch [20/30] Avg Loss: 37.1234\n",
      "Epoch [590/900] completed. Average Loss:  37.4005\n",
      "Epoch [591/900], Batch [20/30] Avg Loss: 39.9970\n",
      "Epoch [591/900] completed. Average Loss:  39.3291\n",
      "Epoch [592/900], Batch [20/30] Avg Loss: 41.7712\n",
      "Epoch [592/900] completed. Average Loss:  40.8041\n",
      "Epoch [593/900], Batch [20/30] Avg Loss: 36.8578\n",
      "Epoch [593/900] completed. Average Loss:  35.9289\n",
      "Epoch [594/900], Batch [20/30] Avg Loss: 38.3573\n",
      "Epoch [594/900] completed. Average Loss:  39.4045\n",
      "Epoch [595/900], Batch [20/30] Avg Loss: 43.2025\n",
      "Epoch [595/900] completed. Average Loss:  42.2479\n",
      "Epoch [596/900], Batch [20/30] Avg Loss: 40.2178\n",
      "Epoch [596/900] completed. Average Loss:  38.8735\n",
      "Epoch [597/900], Batch [20/30] Avg Loss: 42.5141\n",
      "Epoch [597/900] completed. Average Loss:  40.8498\n",
      "Epoch [598/900], Batch [20/30] Avg Loss: 37.4512\n",
      "Epoch [598/900] completed. Average Loss:  39.5340\n",
      "Epoch [599/900], Batch [20/30] Avg Loss: 42.3798\n",
      "Epoch [599/900] completed. Average Loss:  40.1575\n",
      "Epoch [600/900], Batch [20/30] Avg Loss: 39.8144\n",
      "Epoch [600/900] completed. Average Loss:  36.6793\n",
      "Epoch [601/900], Batch [20/30] Avg Loss: 37.9060\n",
      "Epoch [601/900] completed. Average Loss:  38.2713\n",
      "Epoch [602/900], Batch [20/30] Avg Loss: 39.8950\n",
      "Epoch [602/900] completed. Average Loss:  37.4340\n",
      "Epoch [603/900], Batch [20/30] Avg Loss: 40.0080\n",
      "Epoch [603/900] completed. Average Loss:  38.4136\n",
      "Epoch [604/900], Batch [20/30] Avg Loss: 42.7433\n",
      "Epoch [604/900] completed. Average Loss:  40.2670\n",
      "Epoch [605/900], Batch [20/30] Avg Loss: 36.2649\n",
      "Epoch [605/900] completed. Average Loss:  36.6542\n",
      "Epoch [606/900], Batch [20/30] Avg Loss: 37.4320\n",
      "Epoch [606/900] completed. Average Loss:  37.9598\n",
      "Epoch [607/900], Batch [20/30] Avg Loss: 39.7974\n",
      "Epoch [607/900] completed. Average Loss:  37.6645\n",
      "Epoch [608/900], Batch [20/30] Avg Loss: 37.4845\n",
      "Epoch [608/900] completed. Average Loss:  36.6762\n",
      "Epoch [609/900], Batch [20/30] Avg Loss: 36.7214\n",
      "Epoch [609/900] completed. Average Loss:  38.4054\n",
      "Epoch [610/900], Batch [20/30] Avg Loss: 38.3543\n",
      "Epoch [610/900] completed. Average Loss:  38.9882\n",
      "Epoch [611/900], Batch [20/30] Avg Loss: 41.8691\n",
      "Epoch [611/900] completed. Average Loss:  39.1575\n",
      "Epoch [612/900], Batch [20/30] Avg Loss: 40.4425\n",
      "Epoch [612/900] completed. Average Loss:  42.7764\n",
      "Epoch [613/900], Batch [20/30] Avg Loss: 40.3870\n",
      "Epoch [613/900] completed. Average Loss:  39.8770\n",
      "Epoch [614/900], Batch [20/30] Avg Loss: 44.1680\n",
      "Epoch [614/900] completed. Average Loss:  38.6348\n",
      "Epoch [615/900], Batch [20/30] Avg Loss: 39.3192\n",
      "Epoch [615/900] completed. Average Loss:  38.5356\n",
      "Epoch [616/900], Batch [20/30] Avg Loss: 34.4217\n",
      "Epoch [616/900] completed. Average Loss:  35.8087\n",
      "Epoch [617/900], Batch [20/30] Avg Loss: 37.1030\n",
      "Epoch [617/900] completed. Average Loss:  38.7918\n",
      "Epoch [618/900], Batch [20/30] Avg Loss: 39.0671\n",
      "Epoch [618/900] completed. Average Loss:  39.7067\n",
      "Epoch [619/900], Batch [20/30] Avg Loss: 39.1852\n",
      "Epoch [619/900] completed. Average Loss:  39.1240\n",
      "Epoch [620/900], Batch [20/30] Avg Loss: 41.0897\n",
      "Epoch [620/900] completed. Average Loss:  38.6228\n",
      "Epoch [621/900], Batch [20/30] Avg Loss: 41.0040\n",
      "Epoch [621/900] completed. Average Loss:  38.6233\n",
      "Epoch [622/900], Batch [20/30] Avg Loss: 36.5845\n",
      "Epoch [622/900] completed. Average Loss:  38.1954\n",
      "Epoch [623/900], Batch [20/30] Avg Loss: 35.0993\n",
      "Epoch [623/900] completed. Average Loss:  34.5735\n",
      "Epoch [624/900], Batch [20/30] Avg Loss: 34.3937\n",
      "Epoch [624/900] completed. Average Loss:  32.9644\n",
      "Epoch [625/900], Batch [20/30] Avg Loss: 37.5246\n",
      "Epoch [625/900] completed. Average Loss:  39.1747\n",
      "Epoch [626/900], Batch [20/30] Avg Loss: 34.3924\n",
      "Epoch [626/900] completed. Average Loss:  33.8056\n",
      "Epoch [627/900], Batch [20/30] Avg Loss: 35.2698\n",
      "Epoch [627/900] completed. Average Loss:  35.6117\n",
      "Epoch [628/900], Batch [20/30] Avg Loss: 35.8377\n",
      "Epoch [628/900] completed. Average Loss:  34.8305\n",
      "Epoch [629/900], Batch [20/30] Avg Loss: 34.7458\n",
      "Epoch [629/900] completed. Average Loss:  34.2189\n",
      "Epoch [630/900], Batch [20/30] Avg Loss: 38.8725\n",
      "Epoch [630/900] completed. Average Loss:  39.9879\n",
      "Epoch [631/900], Batch [20/30] Avg Loss: 36.4334\n",
      "Epoch [631/900] completed. Average Loss:  36.8219\n",
      "Epoch [632/900], Batch [20/30] Avg Loss: 38.3544\n",
      "Epoch [632/900] completed. Average Loss:  39.7544\n",
      "Epoch [633/900], Batch [20/30] Avg Loss: 41.6726\n",
      "Epoch [633/900] completed. Average Loss:  39.5150\n",
      "Epoch [634/900], Batch [20/30] Avg Loss: 34.5045\n",
      "Epoch [634/900] completed. Average Loss:  37.8373\n",
      "Epoch [635/900], Batch [20/30] Avg Loss: 38.9545\n",
      "Epoch [635/900] completed. Average Loss:  36.7475\n",
      "Epoch [636/900], Batch [20/30] Avg Loss: 38.4666\n",
      "Epoch [636/900] completed. Average Loss:  38.3745\n",
      "Epoch [637/900], Batch [20/30] Avg Loss: 38.1906\n",
      "Epoch [637/900] completed. Average Loss:  36.8702\n",
      "Epoch [638/900], Batch [20/30] Avg Loss: 41.9316\n",
      "Epoch [638/900] completed. Average Loss:  40.2398\n",
      "Epoch [639/900], Batch [20/30] Avg Loss: 34.8207\n",
      "Epoch [639/900] completed. Average Loss:  34.1155\n",
      "Epoch [640/900], Batch [20/30] Avg Loss: 35.3374\n",
      "Epoch [640/900] completed. Average Loss:  35.4065\n",
      "Epoch [641/900], Batch [20/30] Avg Loss: 42.5424\n",
      "Epoch [641/900] completed. Average Loss:  43.0922\n",
      "Epoch [642/900], Batch [20/30] Avg Loss: 35.5983\n",
      "Epoch [642/900] completed. Average Loss:  36.3753\n",
      "Epoch [643/900], Batch [20/30] Avg Loss: 39.1549\n",
      "Epoch [643/900] completed. Average Loss:  38.3629\n",
      "Epoch [644/900], Batch [20/30] Avg Loss: 41.8408\n",
      "Epoch [644/900] completed. Average Loss:  40.0162\n",
      "Epoch [645/900], Batch [20/30] Avg Loss: 34.4428\n",
      "Epoch [645/900] completed. Average Loss:  35.5371\n",
      "Epoch [646/900], Batch [20/30] Avg Loss: 35.8449\n",
      "Epoch [646/900] completed. Average Loss:  35.8252\n",
      "Epoch [647/900], Batch [20/30] Avg Loss: 36.6758\n",
      "Epoch [647/900] completed. Average Loss:  37.9045\n",
      "Epoch [648/900], Batch [20/30] Avg Loss: 34.2852\n",
      "Epoch [648/900] completed. Average Loss:  35.1164\n",
      "Epoch [649/900], Batch [20/30] Avg Loss: 44.6853\n",
      "Epoch [649/900] completed. Average Loss:  40.9113\n",
      "Epoch [650/900], Batch [20/30] Avg Loss: 38.1869\n",
      "Epoch [650/900] completed. Average Loss:  39.4058\n",
      "Epoch [651/900], Batch [20/30] Avg Loss: 36.4961\n",
      "Epoch [651/900] completed. Average Loss:  37.5734\n",
      "Epoch [652/900], Batch [20/30] Avg Loss: 39.1881\n",
      "Epoch [652/900] completed. Average Loss:  37.0320\n",
      "Epoch [653/900], Batch [20/30] Avg Loss: 38.5786\n",
      "Epoch [653/900] completed. Average Loss:  38.5114\n",
      "Epoch [654/900], Batch [20/30] Avg Loss: 40.7464\n",
      "Epoch [654/900] completed. Average Loss:  39.5881\n",
      "Epoch [655/900], Batch [20/30] Avg Loss: 34.3951\n",
      "Epoch [655/900] completed. Average Loss:  34.3198\n",
      "Epoch [656/900], Batch [20/30] Avg Loss: 36.2955\n",
      "Epoch [656/900] completed. Average Loss:  35.4666\n",
      "Epoch [657/900], Batch [20/30] Avg Loss: 41.7161\n",
      "Epoch [657/900] completed. Average Loss:  43.4271\n",
      "Epoch [658/900], Batch [20/30] Avg Loss: 36.8019\n",
      "Epoch [658/900] completed. Average Loss:  35.9983\n",
      "Epoch [659/900], Batch [20/30] Avg Loss: 38.1942\n",
      "Epoch [659/900] completed. Average Loss:  37.5694\n",
      "Epoch [660/900], Batch [20/30] Avg Loss: 34.9901\n",
      "Epoch [660/900] completed. Average Loss:  33.6594\n",
      "Epoch [661/900], Batch [20/30] Avg Loss: 38.2056\n",
      "Epoch [661/900] completed. Average Loss:  38.9715\n",
      "Epoch [662/900], Batch [20/30] Avg Loss: 36.3134\n",
      "Epoch [662/900] completed. Average Loss:  37.1841\n",
      "Epoch [663/900], Batch [20/30] Avg Loss: 35.8664\n",
      "Epoch [663/900] completed. Average Loss:  35.4984\n",
      "Epoch [664/900], Batch [20/30] Avg Loss: 34.8289\n",
      "Epoch [664/900] completed. Average Loss:  34.8113\n",
      "Epoch [665/900], Batch [20/30] Avg Loss: 34.3347\n",
      "Epoch [665/900] completed. Average Loss:  35.4982\n",
      "Epoch [666/900], Batch [20/30] Avg Loss: 37.7975\n",
      "Epoch [666/900] completed. Average Loss:  36.9533\n",
      "Epoch [667/900], Batch [20/30] Avg Loss: 35.4259\n",
      "Epoch [667/900] completed. Average Loss:  34.5309\n",
      "Epoch [668/900], Batch [20/30] Avg Loss: 34.2367\n",
      "Epoch [668/900] completed. Average Loss:  37.5847\n",
      "Epoch [669/900], Batch [20/30] Avg Loss: 38.0828\n",
      "Epoch [669/900] completed. Average Loss:  36.2418\n",
      "Epoch [670/900], Batch [20/30] Avg Loss: 35.6124\n",
      "Epoch [670/900] completed. Average Loss:  39.9207\n",
      "Epoch [671/900], Batch [20/30] Avg Loss: 40.4035\n",
      "Epoch [671/900] completed. Average Loss:  41.5176\n",
      "Epoch [672/900], Batch [20/30] Avg Loss: 35.9799\n",
      "Epoch [672/900] completed. Average Loss:  37.8636\n",
      "Epoch [673/900], Batch [20/30] Avg Loss: 38.9926\n",
      "Epoch [673/900] completed. Average Loss:  37.5811\n",
      "Epoch [674/900], Batch [20/30] Avg Loss: 35.8351\n",
      "Epoch [674/900] completed. Average Loss:  37.2885\n",
      "Epoch [675/900], Batch [20/30] Avg Loss: 41.0617\n",
      "Epoch [675/900] completed. Average Loss:  39.4053\n",
      "Epoch [676/900], Batch [20/30] Avg Loss: 34.6239\n",
      "Epoch [676/900] completed. Average Loss:  35.2357\n",
      "Epoch [677/900], Batch [20/30] Avg Loss: 35.4574\n",
      "Epoch [677/900] completed. Average Loss:  38.6112\n",
      "Epoch [678/900], Batch [20/30] Avg Loss: 37.6247\n",
      "Epoch [678/900] completed. Average Loss:  35.7419\n",
      "Epoch [679/900], Batch [20/30] Avg Loss: 34.7601\n",
      "Epoch [679/900] completed. Average Loss:  35.8868\n",
      "Epoch [680/900], Batch [20/30] Avg Loss: 46.8495\n",
      "Epoch [680/900] completed. Average Loss:  43.1093\n",
      "Epoch [681/900], Batch [20/30] Avg Loss: 36.0133\n",
      "Epoch [681/900] completed. Average Loss:  38.0835\n",
      "Epoch [682/900], Batch [20/30] Avg Loss: 35.4010\n",
      "Epoch [682/900] completed. Average Loss:  34.9046\n",
      "Epoch [683/900], Batch [20/30] Avg Loss: 43.7341\n",
      "Epoch [683/900] completed. Average Loss:  43.2970\n",
      "Epoch [684/900], Batch [20/30] Avg Loss: 38.6857\n",
      "Epoch [684/900] completed. Average Loss:  38.1064\n",
      "Epoch [685/900], Batch [20/30] Avg Loss: 33.3848\n",
      "Epoch [685/900] completed. Average Loss:  35.4076\n",
      "Epoch [686/900], Batch [20/30] Avg Loss: 38.3493\n",
      "Epoch [686/900] completed. Average Loss:  36.1694\n",
      "Epoch [687/900], Batch [20/30] Avg Loss: 30.5241\n",
      "Epoch [687/900] completed. Average Loss:  33.8535\n",
      "Epoch [688/900], Batch [20/30] Avg Loss: 37.8354\n",
      "Epoch [688/900] completed. Average Loss:  38.8803\n",
      "Epoch [689/900], Batch [20/30] Avg Loss: 41.0020\n",
      "Epoch [689/900] completed. Average Loss:  41.2474\n",
      "Epoch [690/900], Batch [20/30] Avg Loss: 37.1809\n",
      "Epoch [690/900] completed. Average Loss:  37.2927\n",
      "Epoch [691/900], Batch [20/30] Avg Loss: 39.2302\n",
      "Epoch [691/900] completed. Average Loss:  37.3380\n",
      "Epoch [692/900], Batch [20/30] Avg Loss: 38.5597\n",
      "Epoch [692/900] completed. Average Loss:  38.1538\n",
      "Epoch [693/900], Batch [20/30] Avg Loss: 44.5072\n",
      "Epoch [693/900] completed. Average Loss:  41.9282\n",
      "Epoch [694/900], Batch [20/30] Avg Loss: 38.8268\n",
      "Epoch [694/900] completed. Average Loss:  41.0836\n",
      "Epoch [695/900], Batch [20/30] Avg Loss: 36.0360\n",
      "Epoch [695/900] completed. Average Loss:  38.0732\n",
      "Epoch [696/900], Batch [20/30] Avg Loss: 36.4878\n",
      "Epoch [696/900] completed. Average Loss:  34.8739\n",
      "Epoch [697/900], Batch [20/30] Avg Loss: 37.4191\n",
      "Epoch [697/900] completed. Average Loss:  38.9772\n",
      "Epoch [698/900], Batch [20/30] Avg Loss: 42.0945\n",
      "Epoch [698/900] completed. Average Loss:  41.1289\n",
      "Epoch [699/900], Batch [20/30] Avg Loss: 41.9120\n",
      "Epoch [699/900] completed. Average Loss:  43.2645\n",
      "Epoch [700/900], Batch [20/30] Avg Loss: 35.6707\n",
      "Epoch [700/900] completed. Average Loss:  35.3303\n",
      "Epoch [701/900], Batch [20/30] Avg Loss: 35.5475\n",
      "Epoch [701/900] completed. Average Loss:  36.4289\n",
      "Epoch [702/900], Batch [20/30] Avg Loss: 33.2936\n",
      "Epoch [702/900] completed. Average Loss:  33.7919\n",
      "Epoch [703/900], Batch [20/30] Avg Loss: 37.6304\n",
      "Epoch [703/900] completed. Average Loss:  35.9018\n",
      "Epoch [704/900], Batch [20/30] Avg Loss: 39.2516\n",
      "Epoch [704/900] completed. Average Loss:  38.3225\n",
      "Epoch [705/900], Batch [20/30] Avg Loss: 34.1439\n",
      "Epoch [705/900] completed. Average Loss:  36.9821\n",
      "Epoch [706/900], Batch [20/30] Avg Loss: 37.9100\n",
      "Epoch [706/900] completed. Average Loss:  36.6521\n",
      "Epoch [707/900], Batch [20/30] Avg Loss: 40.7305\n",
      "Epoch [707/900] completed. Average Loss:  37.5508\n",
      "Epoch [708/900], Batch [20/30] Avg Loss: 35.9336\n",
      "Epoch [708/900] completed. Average Loss:  35.8189\n",
      "Epoch [709/900], Batch [20/30] Avg Loss: 37.6996\n",
      "Epoch [709/900] completed. Average Loss:  33.2247\n",
      "Epoch [710/900], Batch [20/30] Avg Loss: 36.9911\n",
      "Epoch [710/900] completed. Average Loss:  36.9196\n",
      "Epoch [711/900], Batch [20/30] Avg Loss: 38.7128\n",
      "Epoch [711/900] completed. Average Loss:  38.0589\n",
      "Epoch [712/900], Batch [20/30] Avg Loss: 35.8660\n",
      "Epoch [712/900] completed. Average Loss:  35.7771\n",
      "Epoch [713/900], Batch [20/30] Avg Loss: 33.9787\n",
      "Epoch [713/900] completed. Average Loss:  32.2096\n",
      "Epoch [714/900], Batch [20/30] Avg Loss: 41.7473\n",
      "Epoch [714/900] completed. Average Loss:  43.0468\n",
      "Epoch [715/900], Batch [20/30] Avg Loss: 36.3122\n",
      "Epoch [715/900] completed. Average Loss:  36.9379\n",
      "Epoch [716/900], Batch [20/30] Avg Loss: 30.8525\n",
      "Epoch [716/900] completed. Average Loss:  33.0034\n",
      "Epoch [717/900], Batch [20/30] Avg Loss: 42.4280\n",
      "Epoch [717/900] completed. Average Loss:  43.6914\n",
      "Epoch [718/900], Batch [20/30] Avg Loss: 41.6225\n",
      "Epoch [718/900] completed. Average Loss:  41.2151\n",
      "Epoch [719/900], Batch [20/30] Avg Loss: 38.7578\n",
      "Epoch [719/900] completed. Average Loss:  38.8721\n",
      "Epoch [720/900], Batch [20/30] Avg Loss: 40.0180\n",
      "Epoch [720/900] completed. Average Loss:  35.7389\n",
      "Epoch [721/900], Batch [20/30] Avg Loss: 37.6679\n",
      "Epoch [721/900] completed. Average Loss:  36.7377\n",
      "Epoch [722/900], Batch [20/30] Avg Loss: 39.4054\n",
      "Epoch [722/900] completed. Average Loss:  38.3371\n",
      "Epoch [723/900], Batch [20/30] Avg Loss: 38.6577\n",
      "Epoch [723/900] completed. Average Loss:  38.8367\n",
      "Epoch [724/900], Batch [20/30] Avg Loss: 40.7559\n",
      "Epoch [724/900] completed. Average Loss:  40.4796\n",
      "Epoch [725/900], Batch [20/30] Avg Loss: 35.5026\n",
      "Epoch [725/900] completed. Average Loss:  36.3433\n",
      "Epoch [726/900], Batch [20/30] Avg Loss: 38.8403\n",
      "Epoch [726/900] completed. Average Loss:  38.0255\n",
      "Epoch [727/900], Batch [20/30] Avg Loss: 39.3485\n",
      "Epoch [727/900] completed. Average Loss:  39.9764\n",
      "Epoch [728/900], Batch [20/30] Avg Loss: 35.7212\n",
      "Epoch [728/900] completed. Average Loss:  34.7388\n",
      "Epoch [729/900], Batch [20/30] Avg Loss: 33.3620\n",
      "Epoch [729/900] completed. Average Loss:  32.8373\n",
      "Epoch [730/900], Batch [20/30] Avg Loss: 33.7036\n",
      "Epoch [730/900] completed. Average Loss:  34.6012\n",
      "Epoch [731/900], Batch [20/30] Avg Loss: 33.2744\n",
      "Epoch [731/900] completed. Average Loss:  34.3770\n",
      "Epoch [732/900], Batch [20/30] Avg Loss: 35.1715\n",
      "Epoch [732/900] completed. Average Loss:  35.2655\n",
      "Epoch [733/900], Batch [20/30] Avg Loss: 42.0832\n",
      "Epoch [733/900] completed. Average Loss:  40.0176\n",
      "Epoch [734/900], Batch [20/30] Avg Loss: 49.2514\n",
      "Epoch [734/900] completed. Average Loss:  45.3687\n",
      "Epoch [735/900], Batch [20/30] Avg Loss: 41.9097\n",
      "Epoch [735/900] completed. Average Loss:  39.6171\n",
      "Epoch [736/900], Batch [20/30] Avg Loss: 37.1821\n",
      "Epoch [736/900] completed. Average Loss:  37.9966\n",
      "Epoch [737/900], Batch [20/30] Avg Loss: 34.8224\n",
      "Epoch [737/900] completed. Average Loss:  37.9646\n",
      "Epoch [738/900], Batch [20/30] Avg Loss: 39.8670\n",
      "Epoch [738/900] completed. Average Loss:  39.4543\n",
      "Epoch [739/900], Batch [20/30] Avg Loss: 34.1570\n",
      "Epoch [739/900] completed. Average Loss:  33.5881\n",
      "Epoch [740/900], Batch [20/30] Avg Loss: 39.7567\n",
      "Epoch [740/900] completed. Average Loss:  37.3097\n",
      "Epoch [741/900], Batch [20/30] Avg Loss: 42.5239\n",
      "Epoch [741/900] completed. Average Loss:  38.6377\n",
      "Epoch [742/900], Batch [20/30] Avg Loss: 37.1707\n",
      "Epoch [742/900] completed. Average Loss:  35.6713\n",
      "Epoch [743/900], Batch [20/30] Avg Loss: 42.1582\n",
      "Epoch [743/900] completed. Average Loss:  41.8778\n",
      "Epoch [744/900], Batch [20/30] Avg Loss: 37.8886\n",
      "Epoch [744/900] completed. Average Loss:  38.9348\n",
      "Epoch [745/900], Batch [20/30] Avg Loss: 36.3321\n",
      "Epoch [745/900] completed. Average Loss:  36.2825\n",
      "Epoch [746/900], Batch [20/30] Avg Loss: 35.9440\n",
      "Epoch [746/900] completed. Average Loss:  39.1873\n",
      "Epoch [747/900], Batch [20/30] Avg Loss: 41.1907\n",
      "Epoch [747/900] completed. Average Loss:  39.0635\n",
      "Epoch [748/900], Batch [20/30] Avg Loss: 36.6705\n",
      "Epoch [748/900] completed. Average Loss:  32.5806\n",
      "Epoch [749/900], Batch [20/30] Avg Loss: 39.5072\n",
      "Epoch [749/900] completed. Average Loss:  39.9950\n",
      "Epoch [750/900], Batch [20/30] Avg Loss: 34.1165\n",
      "Epoch [750/900] completed. Average Loss:  34.8418\n",
      "Epoch [751/900], Batch [20/30] Avg Loss: 34.5306\n",
      "Epoch [751/900] completed. Average Loss:  33.6300\n",
      "Epoch [752/900], Batch [20/30] Avg Loss: 32.2694\n",
      "Epoch [752/900] completed. Average Loss:  34.7777\n",
      "Epoch [753/900], Batch [20/30] Avg Loss: 35.1219\n",
      "Epoch [753/900] completed. Average Loss:  37.0187\n",
      "Epoch [754/900], Batch [20/30] Avg Loss: 31.5190\n",
      "Epoch [754/900] completed. Average Loss:  32.4480\n",
      "Epoch [755/900], Batch [20/30] Avg Loss: 35.9645\n",
      "Epoch [755/900] completed. Average Loss:  38.0711\n",
      "Epoch [756/900], Batch [20/30] Avg Loss: 38.5278\n",
      "Epoch [756/900] completed. Average Loss:  37.8258\n",
      "Epoch [757/900], Batch [20/30] Avg Loss: 38.3719\n",
      "Epoch [757/900] completed. Average Loss:  39.4212\n",
      "Epoch [758/900], Batch [20/30] Avg Loss: 38.3109\n",
      "Epoch [758/900] completed. Average Loss:  38.3574\n",
      "Epoch [759/900], Batch [20/30] Avg Loss: 38.5654\n",
      "Epoch [759/900] completed. Average Loss:  37.1444\n",
      "Epoch [760/900], Batch [20/30] Avg Loss: 38.5630\n",
      "Epoch [760/900] completed. Average Loss:  36.6267\n",
      "Epoch [761/900], Batch [20/30] Avg Loss: 36.7036\n",
      "Epoch [761/900] completed. Average Loss:  36.7230\n",
      "Epoch [762/900], Batch [20/30] Avg Loss: 36.4366\n",
      "Epoch [762/900] completed. Average Loss:  35.6152\n",
      "Epoch [763/900], Batch [20/30] Avg Loss: 36.7618\n",
      "Epoch [763/900] completed. Average Loss:  37.5388\n",
      "Epoch [764/900], Batch [20/30] Avg Loss: 37.4846\n",
      "Epoch [764/900] completed. Average Loss:  36.7701\n",
      "Epoch [765/900], Batch [20/30] Avg Loss: 40.0179\n",
      "Epoch [765/900] completed. Average Loss:  37.2925\n",
      "Epoch [766/900], Batch [20/30] Avg Loss: 36.2884\n",
      "Epoch [766/900] completed. Average Loss:  37.2931\n",
      "Epoch [767/900], Batch [20/30] Avg Loss: 38.1239\n",
      "Epoch [767/900] completed. Average Loss:  36.9721\n",
      "Epoch [768/900], Batch [20/30] Avg Loss: 41.4680\n",
      "Epoch [768/900] completed. Average Loss:  40.7670\n",
      "Epoch [769/900], Batch [20/30] Avg Loss: 36.8234\n",
      "Epoch [769/900] completed. Average Loss:  37.2621\n",
      "Epoch [770/900], Batch [20/30] Avg Loss: 38.9241\n",
      "Epoch [770/900] completed. Average Loss:  39.2497\n",
      "Epoch [771/900], Batch [20/30] Avg Loss: 35.6551\n",
      "Epoch [771/900] completed. Average Loss:  35.0701\n",
      "Epoch [772/900], Batch [20/30] Avg Loss: 36.3272\n",
      "Epoch [772/900] completed. Average Loss:  36.8339\n",
      "Epoch [773/900], Batch [20/30] Avg Loss: 35.4540\n",
      "Epoch [773/900] completed. Average Loss:  37.9104\n",
      "Epoch [774/900], Batch [20/30] Avg Loss: 36.4075\n",
      "Epoch [774/900] completed. Average Loss:  36.4279\n",
      "Epoch [775/900], Batch [20/30] Avg Loss: 33.4797\n",
      "Epoch [775/900] completed. Average Loss:  35.0749\n",
      "Epoch [776/900], Batch [20/30] Avg Loss: 37.8877\n",
      "Epoch [776/900] completed. Average Loss:  36.1417\n",
      "Epoch [777/900], Batch [20/30] Avg Loss: 35.2173\n",
      "Epoch [777/900] completed. Average Loss:  34.5220\n",
      "Epoch [778/900], Batch [20/30] Avg Loss: 35.7529\n",
      "Epoch [778/900] completed. Average Loss:  36.3676\n",
      "Epoch [779/900], Batch [20/30] Avg Loss: 39.7398\n",
      "Epoch [779/900] completed. Average Loss:  38.2952\n",
      "Epoch [780/900], Batch [20/30] Avg Loss: 35.7216\n",
      "Epoch [780/900] completed. Average Loss:  38.2562\n",
      "Epoch [781/900], Batch [20/30] Avg Loss: 40.5229\n",
      "Epoch [781/900] completed. Average Loss:  39.0042\n",
      "Epoch [782/900], Batch [20/30] Avg Loss: 34.5817\n",
      "Epoch [782/900] completed. Average Loss:  36.2374\n",
      "Epoch [783/900], Batch [20/30] Avg Loss: 38.5262\n",
      "Epoch [783/900] completed. Average Loss:  39.6669\n",
      "Epoch [784/900], Batch [20/30] Avg Loss: 40.1553\n",
      "Epoch [784/900] completed. Average Loss:  40.2688\n",
      "Epoch [785/900], Batch [20/30] Avg Loss: 36.4665\n",
      "Epoch [785/900] completed. Average Loss:  37.4531\n",
      "Epoch [786/900], Batch [20/30] Avg Loss: 33.6653\n",
      "Epoch [786/900] completed. Average Loss:  36.9389\n",
      "Epoch [787/900], Batch [20/30] Avg Loss: 35.4699\n",
      "Epoch [787/900] completed. Average Loss:  34.3146\n",
      "Epoch [788/900], Batch [20/30] Avg Loss: 42.2850\n",
      "Epoch [788/900] completed. Average Loss:  40.1989\n",
      "Epoch [789/900], Batch [20/30] Avg Loss: 40.4882\n",
      "Epoch [789/900] completed. Average Loss:  38.4437\n",
      "Epoch [790/900], Batch [20/30] Avg Loss: 36.3996\n",
      "Epoch [790/900] completed. Average Loss:  36.3179\n",
      "Epoch [791/900], Batch [20/30] Avg Loss: 35.4775\n",
      "Epoch [791/900] completed. Average Loss:  37.2977\n",
      "Epoch [792/900], Batch [20/30] Avg Loss: 36.2486\n",
      "Epoch [792/900] completed. Average Loss:  37.0204\n",
      "Epoch [793/900], Batch [20/30] Avg Loss: 32.4538\n",
      "Epoch [793/900] completed. Average Loss:  34.3772\n",
      "Epoch [794/900], Batch [20/30] Avg Loss: 29.8979\n",
      "Epoch [794/900] completed. Average Loss:  31.5325\n",
      "Epoch [795/900], Batch [20/30] Avg Loss: 44.4127\n",
      "Epoch [795/900] completed. Average Loss:  43.0200\n",
      "Epoch [796/900], Batch [20/30] Avg Loss: 37.4845\n",
      "Epoch [796/900] completed. Average Loss:  36.4043\n",
      "Epoch [797/900], Batch [20/30] Avg Loss: 37.6757\n",
      "Epoch [797/900] completed. Average Loss:  38.1578\n",
      "Epoch [798/900], Batch [20/30] Avg Loss: 37.1754\n",
      "Epoch [798/900] completed. Average Loss:  37.2608\n",
      "Epoch [799/900], Batch [20/30] Avg Loss: 36.6790\n",
      "Epoch [799/900] completed. Average Loss:  38.2591\n",
      "Epoch [800/900], Batch [20/30] Avg Loss: 36.0025\n",
      "Epoch [800/900] completed. Average Loss:  36.6129\n",
      "Epoch [801/900], Batch [20/30] Avg Loss: 34.7463\n",
      "Epoch [801/900] completed. Average Loss:  35.4855\n",
      "Epoch [802/900], Batch [20/30] Avg Loss: 42.6038\n",
      "Epoch [802/900] completed. Average Loss:  40.2754\n",
      "Epoch [803/900], Batch [20/30] Avg Loss: 36.1662\n",
      "Epoch [803/900] completed. Average Loss:  33.6149\n",
      "Epoch [804/900], Batch [20/30] Avg Loss: 37.1623\n",
      "Epoch [804/900] completed. Average Loss:  38.2592\n",
      "Epoch [805/900], Batch [20/30] Avg Loss: 37.5286\n",
      "Epoch [805/900] completed. Average Loss:  36.7034\n",
      "Epoch [806/900], Batch [20/30] Avg Loss: 29.7703\n",
      "Epoch [806/900] completed. Average Loss:  32.6223\n",
      "Epoch [807/900], Batch [20/30] Avg Loss: 38.4797\n",
      "Epoch [807/900] completed. Average Loss:  37.2485\n",
      "Epoch [808/900], Batch [20/30] Avg Loss: 37.4924\n",
      "Epoch [808/900] completed. Average Loss:  36.2816\n",
      "Epoch [809/900], Batch [20/30] Avg Loss: 34.5051\n",
      "Epoch [809/900] completed. Average Loss:  35.2523\n",
      "Epoch [810/900], Batch [20/30] Avg Loss: 36.5031\n",
      "Epoch [810/900] completed. Average Loss:  37.5185\n",
      "Epoch [811/900], Batch [20/30] Avg Loss: 37.8822\n",
      "Epoch [811/900] completed. Average Loss:  39.2748\n",
      "Epoch [812/900], Batch [20/30] Avg Loss: 35.8513\n",
      "Epoch [812/900] completed. Average Loss:  34.7897\n",
      "Epoch [813/900], Batch [20/30] Avg Loss: 36.9459\n",
      "Epoch [813/900] completed. Average Loss:  36.4485\n",
      "Epoch [814/900], Batch [20/30] Avg Loss: 38.4863\n",
      "Epoch [814/900] completed. Average Loss:  38.7522\n",
      "Epoch [815/900], Batch [20/30] Avg Loss: 38.5177\n",
      "Epoch [815/900] completed. Average Loss:  38.7789\n",
      "Epoch [816/900], Batch [20/30] Avg Loss: 33.3477\n",
      "Epoch [816/900] completed. Average Loss:  35.7335\n",
      "Epoch [817/900], Batch [20/30] Avg Loss: 35.4370\n",
      "Epoch [817/900] completed. Average Loss:  34.9387\n",
      "Epoch [818/900], Batch [20/30] Avg Loss: 35.9717\n",
      "Epoch [818/900] completed. Average Loss:  35.7300\n",
      "Epoch [819/900], Batch [20/30] Avg Loss: 37.4688\n",
      "Epoch [819/900] completed. Average Loss:  37.9192\n",
      "Epoch [820/900], Batch [20/30] Avg Loss: 38.2276\n",
      "Epoch [820/900] completed. Average Loss:  37.5276\n",
      "Epoch [821/900], Batch [20/30] Avg Loss: 36.1487\n",
      "Epoch [821/900] completed. Average Loss:  36.0393\n",
      "Epoch [822/900], Batch [20/30] Avg Loss: 32.6385\n",
      "Epoch [822/900] completed. Average Loss:  34.6369\n",
      "Epoch [823/900], Batch [20/30] Avg Loss: 38.2167\n",
      "Epoch [823/900] completed. Average Loss:  35.7747\n",
      "Epoch [824/900], Batch [20/30] Avg Loss: 37.6451\n",
      "Epoch [824/900] completed. Average Loss:  38.1565\n",
      "Epoch [825/900], Batch [20/30] Avg Loss: 36.7282\n",
      "Epoch [825/900] completed. Average Loss:  35.7815\n",
      "Epoch [826/900], Batch [20/30] Avg Loss: 34.1503\n",
      "Epoch [826/900] completed. Average Loss:  35.9575\n",
      "Epoch [827/900], Batch [20/30] Avg Loss: 34.3359\n",
      "Epoch [827/900] completed. Average Loss:  33.5348\n",
      "Epoch [828/900], Batch [20/30] Avg Loss: 37.0987\n",
      "Epoch [828/900] completed. Average Loss:  37.5724\n",
      "Epoch [829/900], Batch [20/30] Avg Loss: 34.2500\n",
      "Epoch [829/900] completed. Average Loss:  33.4699\n",
      "Epoch [830/900], Batch [20/30] Avg Loss: 32.9485\n",
      "Epoch [830/900] completed. Average Loss:  32.9423\n",
      "Epoch [831/900], Batch [20/30] Avg Loss: 37.4159\n",
      "Epoch [831/900] completed. Average Loss:  36.7095\n",
      "Epoch [832/900], Batch [20/30] Avg Loss: 32.8511\n",
      "Epoch [832/900] completed. Average Loss:  34.3828\n",
      "Epoch [833/900], Batch [20/30] Avg Loss: 36.0679\n",
      "Epoch [833/900] completed. Average Loss:  36.1137\n",
      "Epoch [834/900], Batch [20/30] Avg Loss: 36.7016\n",
      "Epoch [834/900] completed. Average Loss:  37.8480\n",
      "Epoch [835/900], Batch [20/30] Avg Loss: 35.2617\n",
      "Epoch [835/900] completed. Average Loss:  35.7969\n",
      "Epoch [836/900], Batch [20/30] Avg Loss: 39.0083\n",
      "Epoch [836/900] completed. Average Loss:  37.0927\n",
      "Epoch [837/900], Batch [20/30] Avg Loss: 37.2143\n",
      "Epoch [837/900] completed. Average Loss:  35.3808\n",
      "Epoch [838/900], Batch [20/30] Avg Loss: 31.4034\n",
      "Epoch [838/900] completed. Average Loss:  34.2652\n",
      "Epoch [839/900], Batch [20/30] Avg Loss: 36.7578\n",
      "Epoch [839/900] completed. Average Loss:  36.5231\n",
      "Epoch [840/900], Batch [20/30] Avg Loss: 34.1867\n",
      "Epoch [840/900] completed. Average Loss:  34.7527\n",
      "Epoch [841/900], Batch [20/30] Avg Loss: 36.2353\n",
      "Epoch [841/900] completed. Average Loss:  35.6194\n",
      "Epoch [842/900], Batch [20/30] Avg Loss: 35.5539\n",
      "Epoch [842/900] completed. Average Loss:  38.3746\n",
      "Epoch [843/900], Batch [20/30] Avg Loss: 37.2534\n",
      "Epoch [843/900] completed. Average Loss:  37.2392\n",
      "Epoch [844/900], Batch [20/30] Avg Loss: 36.8734\n",
      "Epoch [844/900] completed. Average Loss:  34.3780\n",
      "Epoch [845/900], Batch [20/30] Avg Loss: 37.2673\n",
      "Epoch [845/900] completed. Average Loss:  35.9691\n",
      "Epoch [846/900], Batch [20/30] Avg Loss: 38.3958\n",
      "Epoch [846/900] completed. Average Loss:  37.4843\n",
      "Epoch [847/900], Batch [20/30] Avg Loss: 33.9571\n",
      "Epoch [847/900] completed. Average Loss:  39.1439\n",
      "Epoch [848/900], Batch [20/30] Avg Loss: 29.1153\n",
      "Epoch [848/900] completed. Average Loss:  32.4048\n",
      "Epoch [849/900], Batch [20/30] Avg Loss: 35.1581\n",
      "Epoch [849/900] completed. Average Loss:  35.0064\n",
      "Epoch [850/900], Batch [20/30] Avg Loss: 33.1872\n",
      "Epoch [850/900] completed. Average Loss:  35.9717\n",
      "Epoch [851/900], Batch [20/30] Avg Loss: 37.6025\n",
      "Epoch [851/900] completed. Average Loss:  35.8667\n",
      "Epoch [852/900], Batch [20/30] Avg Loss: 33.2202\n",
      "Epoch [852/900] completed. Average Loss:  36.3923\n",
      "Epoch [853/900], Batch [20/30] Avg Loss: 37.4412\n",
      "Epoch [853/900] completed. Average Loss:  36.5574\n",
      "Epoch [854/900], Batch [20/30] Avg Loss: 33.2854\n",
      "Epoch [854/900] completed. Average Loss:  36.1373\n",
      "Epoch [855/900], Batch [20/30] Avg Loss: 34.6523\n",
      "Epoch [855/900] completed. Average Loss:  34.5348\n",
      "Epoch [856/900], Batch [20/30] Avg Loss: 39.5655\n",
      "Epoch [856/900] completed. Average Loss:  37.1071\n",
      "Epoch [857/900], Batch [20/30] Avg Loss: 35.6031\n",
      "Epoch [857/900] completed. Average Loss:  36.5405\n",
      "Epoch [858/900], Batch [20/30] Avg Loss: 40.2242\n",
      "Epoch [858/900] completed. Average Loss:  39.4259\n",
      "Epoch [859/900], Batch [20/30] Avg Loss: 35.8355\n",
      "Epoch [859/900] completed. Average Loss:  37.0222\n",
      "Epoch [860/900], Batch [20/30] Avg Loss: 36.2779\n",
      "Epoch [860/900] completed. Average Loss:  36.1037\n",
      "Epoch [861/900], Batch [20/30] Avg Loss: 41.3913\n",
      "Epoch [861/900] completed. Average Loss:  40.2480\n",
      "Epoch [862/900], Batch [20/30] Avg Loss: 37.6483\n",
      "Epoch [862/900] completed. Average Loss:  35.9488\n",
      "Epoch [863/900], Batch [20/30] Avg Loss: 34.1737\n",
      "Epoch [863/900] completed. Average Loss:  33.6811\n",
      "Epoch [864/900], Batch [20/30] Avg Loss: 38.4662\n",
      "Epoch [864/900] completed. Average Loss:  38.7386\n",
      "Epoch [865/900], Batch [20/30] Avg Loss: 33.9925\n",
      "Epoch [865/900] completed. Average Loss:  37.9290\n",
      "Epoch [866/900], Batch [20/30] Avg Loss: 30.1551\n",
      "Epoch [866/900] completed. Average Loss:  31.0528\n",
      "Epoch [867/900], Batch [20/30] Avg Loss: 31.2811\n",
      "Epoch [867/900] completed. Average Loss:  33.4232\n",
      "Epoch [868/900], Batch [20/30] Avg Loss: 36.3367\n",
      "Epoch [868/900] completed. Average Loss:  36.6555\n",
      "Epoch [869/900], Batch [20/30] Avg Loss: 32.4598\n",
      "Epoch [869/900] completed. Average Loss:  33.5222\n",
      "Epoch [870/900], Batch [20/30] Avg Loss: 34.3014\n",
      "Epoch [870/900] completed. Average Loss:  33.6167\n",
      "Epoch [871/900], Batch [20/30] Avg Loss: 34.0509\n",
      "Epoch [871/900] completed. Average Loss:  35.3439\n",
      "Epoch [872/900], Batch [20/30] Avg Loss: 39.3584\n",
      "Epoch [872/900] completed. Average Loss:  38.0711\n",
      "Epoch [873/900], Batch [20/30] Avg Loss: 38.0291\n",
      "Epoch [873/900] completed. Average Loss:  37.3834\n",
      "Epoch [874/900], Batch [20/30] Avg Loss: 36.3045\n",
      "Epoch [874/900] completed. Average Loss:  35.9618\n",
      "Epoch [875/900], Batch [20/30] Avg Loss: 36.7946\n",
      "Epoch [875/900] completed. Average Loss:  35.2934\n",
      "Epoch [876/900], Batch [20/30] Avg Loss: 37.2605\n",
      "Epoch [876/900] completed. Average Loss:  36.6739\n",
      "Epoch [877/900], Batch [20/30] Avg Loss: 34.6989\n",
      "Epoch [877/900] completed. Average Loss:  32.3092\n",
      "Epoch [878/900], Batch [20/30] Avg Loss: 37.3062\n",
      "Epoch [878/900] completed. Average Loss:  35.8992\n",
      "Epoch [879/900], Batch [20/30] Avg Loss: 38.1976\n",
      "Epoch [879/900] completed. Average Loss:  36.2255\n",
      "Epoch [880/900], Batch [20/30] Avg Loss: 30.1160\n",
      "Epoch [880/900] completed. Average Loss:  31.3911\n",
      "Epoch [881/900], Batch [20/30] Avg Loss: 35.6042\n",
      "Epoch [881/900] completed. Average Loss:  34.7369\n",
      "Epoch [882/900], Batch [20/30] Avg Loss: 34.5837\n",
      "Epoch [882/900] completed. Average Loss:  32.8703\n",
      "Epoch [883/900], Batch [20/30] Avg Loss: 32.6896\n",
      "Epoch [883/900] completed. Average Loss:  33.3881\n",
      "Epoch [884/900], Batch [20/30] Avg Loss: 38.7407\n",
      "Epoch [884/900] completed. Average Loss:  36.8335\n",
      "Epoch [885/900], Batch [20/30] Avg Loss: 34.9877\n",
      "Epoch [885/900] completed. Average Loss:  35.3023\n",
      "Epoch [886/900], Batch [20/30] Avg Loss: 41.4790\n",
      "Epoch [886/900] completed. Average Loss:  40.6273\n",
      "Epoch [887/900], Batch [20/30] Avg Loss: 38.9369\n",
      "Epoch [887/900] completed. Average Loss:  37.1608\n",
      "Epoch [888/900], Batch [20/30] Avg Loss: 37.5798\n",
      "Epoch [888/900] completed. Average Loss:  38.2428\n",
      "Epoch [889/900], Batch [20/30] Avg Loss: 33.4198\n",
      "Epoch [889/900] completed. Average Loss:  33.6548\n",
      "Epoch [890/900], Batch [20/30] Avg Loss: 27.7908\n",
      "Epoch [890/900] completed. Average Loss:  31.3043\n",
      "Epoch [891/900], Batch [20/30] Avg Loss: 31.8405\n",
      "Epoch [891/900] completed. Average Loss:  33.4422\n",
      "Epoch [892/900], Batch [20/30] Avg Loss: 35.1427\n",
      "Epoch [892/900] completed. Average Loss:  35.0838\n",
      "Epoch [893/900], Batch [20/30] Avg Loss: 42.0904\n",
      "Epoch [893/900] completed. Average Loss:  38.5464\n",
      "Epoch [894/900], Batch [20/30] Avg Loss: 39.1440\n",
      "Epoch [894/900] completed. Average Loss:  37.9472\n",
      "Epoch [895/900], Batch [20/30] Avg Loss: 35.4238\n",
      "Epoch [895/900] completed. Average Loss:  36.3512\n",
      "Epoch [896/900], Batch [20/30] Avg Loss: 29.8494\n",
      "Epoch [896/900] completed. Average Loss:  31.5187\n",
      "Epoch [897/900], Batch [20/30] Avg Loss: 34.1155\n",
      "Epoch [897/900] completed. Average Loss:  35.6137\n",
      "Epoch [898/900], Batch [20/30] Avg Loss: 36.4177\n",
      "Epoch [898/900] completed. Average Loss:  35.9138\n",
      "Epoch [899/900], Batch [20/30] Avg Loss: 35.0298\n",
      "Epoch [899/900] completed. Average Loss:  32.0210\n",
      "Epoch [900/900], Batch [20/30] Avg Loss: 37.7974\n",
      "Epoch [900/900] completed. Average Loss:  35.9880\n",
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "average_losses, trained_model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(average_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biocomp-NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
